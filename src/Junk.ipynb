{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.5756e-01, -4.5152e-01,  1.5634e+00, -4.4968e-01],\n",
      "        [-5.2195e-03,  7.3769e-01, -5.9260e-01, -1.6914e-01],\n",
      "        [-1.9742e-01, -5.3052e+00,  4.5895e+00,  1.0654e+00],\n",
      "        [ 2.1769e-01,  2.1175e+00, -2.0255e+00, -3.3596e-01]])\n",
      "tensor([[-0.1728, -0.9720,  0.0669,  0.1441],\n",
      "        [ 0.1217, -0.0677, -0.9797,  0.1444],\n",
      "        [-0.9022,  0.2103, -0.0722,  0.3696],\n",
      "        [ 0.3760,  0.0796,  0.1748,  0.9065]])\n",
      "tensor([7.8612e+00, 1.3568e+00, 1.1492e-07, 3.4908e-08])\n",
      "tensor([[ 0.0518,  0.5968,  0.2014, -0.7749],\n",
      "        [ 0.7315, -0.4113, -0.3976, -0.3712],\n",
      "        [-0.6671, -0.4981, -0.2514, -0.4936],\n",
      "        [-0.1311,  0.4760, -0.8592,  0.1345]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "d = 4\n",
    "W_rank = 2\n",
    "k = 4\n",
    "W = torch.randn(d, W_rank) @ torch.randn(W_rank, k)\n",
    "print(W)\n",
    "U, S, V = torch.svd(W)\n",
    "print(U)\n",
    "print(S)\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of W: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "W_rank = np.linalg.matrix_rank(W)\n",
    "print(f'Rank of W: {W_rank}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W@W.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: tensor([ 6.1798e+01+0.j,  1.8408e+00+0.j,  7.5576e-07+0.j, -5.5081e-07+0.j])\n",
      "Eigenvectors: tensor([[-0.1728+0.j, -0.9720+0.j, -0.1181+0.j,  0.1587+0.j],\n",
      "        [ 0.1217+0.j, -0.0677+0.j, -0.4259+0.j, -0.2312+0.j],\n",
      "        [-0.9022+0.j,  0.2103+0.j, -0.3745+0.j,  0.3161+0.j],\n",
      "        [ 0.3760+0.j,  0.0796+0.j, -0.8151+0.j,  0.9063+0.j]])\n"
     ]
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = torch.linalg.eig(W)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\", eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model runs on cuda\n",
      "ComplexModel(\n",
      "  (layer1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (layer2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (layer3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (layer4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Epoch [1/100], Step [100/938], Loss: 1.1655, Accuracy: 59.67%\n",
      "Epoch [1/100], Step [200/938], Loss: 0.8305, Accuracy: 72.30%\n",
      "Epoch [1/100], Step [300/938], Loss: 0.6883, Accuracy: 77.51%\n",
      "Epoch [1/100], Step [400/938], Loss: 0.6050, Accuracy: 80.60%\n",
      "Epoch [1/100], Step [500/938], Loss: 0.5494, Accuracy: 82.66%\n",
      "Epoch [1/100], Step [600/938], Loss: 0.5091, Accuracy: 84.11%\n",
      "Epoch [1/100], Step [700/938], Loss: 0.4790, Accuracy: 85.15%\n",
      "Epoch [1/100], Step [800/938], Loss: 0.4545, Accuracy: 86.01%\n",
      "Epoch [1/100], Step [900/938], Loss: 0.4315, Accuracy: 86.79%\n",
      "Epoch [1/100] completed in 9.82 seconds. Average Loss: 0.4240, Accuracy: 87.02%\n",
      "Test Loss: 0.1335, Test Accuracy: 96.00%\n",
      "\n",
      "Class: 0               - Correct: 971/980 (99.08%)\n",
      "Class: 1               - Correct: 1116/1135 (98.33%)\n",
      "Class: 2               - Correct: 980/1032 (94.96%)\n",
      "Class: 3               - Correct: 977/1010 (96.73%)\n",
      "Class: 4               - Correct: 950/982 (96.74%)\n",
      "Class: 5               - Correct: 849/892 (95.18%)\n",
      "Class: 6               - Correct: 916/958 (95.62%)\n",
      "Class: 7               - Correct: 981/1028 (95.43%)\n",
      "Class: 8               - Correct: 925/974 (94.97%)\n",
      "Class: 9               - Correct: 935/1009 (92.67%)\n",
      "\n",
      "\n",
      "Epoch [2/100], Step [100/938], Loss: 0.2187, Accuracy: 93.83%\n",
      "Epoch [2/100], Step [200/938], Loss: 0.2274, Accuracy: 93.60%\n",
      "Epoch [2/100], Step [300/938], Loss: 0.2361, Accuracy: 93.44%\n",
      "Epoch [2/100], Step [400/938], Loss: 0.2326, Accuracy: 93.49%\n",
      "Epoch [2/100], Step [500/938], Loss: 0.2344, Accuracy: 93.46%\n",
      "Epoch [2/100], Step [600/938], Loss: 0.2329, Accuracy: 93.52%\n",
      "Epoch [2/100], Step [700/938], Loss: 0.2288, Accuracy: 93.61%\n",
      "Epoch [2/100], Step [800/938], Loss: 0.2265, Accuracy: 93.67%\n",
      "Epoch [2/100], Step [900/938], Loss: 0.2234, Accuracy: 93.81%\n",
      "Epoch [2/100] completed in 9.65 seconds. Average Loss: 0.2219, Accuracy: 93.85%\n",
      "Test Loss: 0.1170, Test Accuracy: 96.67%\n",
      "\n",
      "Class: 0               - Correct: 964/980 (98.37%)\n",
      "Class: 1               - Correct: 1124/1135 (99.03%)\n",
      "Class: 2               - Correct: 999/1032 (96.80%)\n",
      "Class: 3               - Correct: 984/1010 (97.43%)\n",
      "Class: 4               - Correct: 938/982 (95.52%)\n",
      "Class: 5               - Correct: 867/892 (97.20%)\n",
      "Class: 6               - Correct: 928/958 (96.87%)\n",
      "Class: 7               - Correct: 997/1028 (96.98%)\n",
      "Class: 8               - Correct: 933/974 (95.79%)\n",
      "Class: 9               - Correct: 933/1009 (92.47%)\n",
      "\n",
      "\n",
      "Epoch [3/100], Step [100/938], Loss: 0.1676, Accuracy: 95.28%\n",
      "Epoch [3/100], Step [200/938], Loss: 0.1734, Accuracy: 95.06%\n",
      "Epoch [3/100], Step [300/938], Loss: 0.1798, Accuracy: 94.92%\n",
      "Epoch [3/100], Step [400/938], Loss: 0.1828, Accuracy: 94.90%\n",
      "Epoch [3/100], Step [500/938], Loss: 0.1799, Accuracy: 95.02%\n",
      "Epoch [3/100], Step [600/938], Loss: 0.1818, Accuracy: 95.01%\n",
      "Epoch [3/100], Step [700/938], Loss: 0.1826, Accuracy: 94.99%\n",
      "Epoch [3/100], Step [800/938], Loss: 0.1844, Accuracy: 94.96%\n",
      "Epoch [3/100], Step [900/938], Loss: 0.1849, Accuracy: 94.95%\n",
      "Epoch [3/100] completed in 9.73 seconds. Average Loss: 0.1862, Accuracy: 94.93%\n",
      "Test Loss: 0.0984, Test Accuracy: 97.02%\n",
      "\n",
      "Class: 0               - Correct: 974/980 (99.39%)\n",
      "Class: 1               - Correct: 1120/1135 (98.68%)\n",
      "Class: 2               - Correct: 1008/1032 (97.67%)\n",
      "Class: 3               - Correct: 990/1010 (98.02%)\n",
      "Class: 4               - Correct: 947/982 (96.44%)\n",
      "Class: 5               - Correct: 840/892 (94.17%)\n",
      "Class: 6               - Correct: 936/958 (97.70%)\n",
      "Class: 7               - Correct: 998/1028 (97.08%)\n",
      "Class: 8               - Correct: 936/974 (96.10%)\n",
      "Class: 9               - Correct: 953/1009 (94.45%)\n",
      "\n",
      "\n",
      "Epoch [4/100], Step [100/938], Loss: 0.1737, Accuracy: 95.34%\n",
      "Epoch [4/100], Step [200/938], Loss: 0.1614, Accuracy: 95.63%\n",
      "Epoch [4/100], Step [300/938], Loss: 0.1658, Accuracy: 95.48%\n",
      "Epoch [4/100], Step [400/938], Loss: 0.1705, Accuracy: 95.41%\n",
      "Epoch [4/100], Step [500/938], Loss: 0.1671, Accuracy: 95.50%\n",
      "Epoch [4/100], Step [600/938], Loss: 0.1674, Accuracy: 95.47%\n",
      "Epoch [4/100], Step [700/938], Loss: 0.1676, Accuracy: 95.44%\n",
      "Epoch [4/100], Step [800/938], Loss: 0.1680, Accuracy: 95.44%\n",
      "Epoch [4/100], Step [900/938], Loss: 0.1687, Accuracy: 95.44%\n",
      "Epoch [4/100] completed in 9.78 seconds. Average Loss: 0.1677, Accuracy: 95.47%\n",
      "Test Loss: 0.1025, Test Accuracy: 97.02%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1128/1135 (99.38%)\n",
      "Class: 2               - Correct: 1008/1032 (97.67%)\n",
      "Class: 3               - Correct: 968/1010 (95.84%)\n",
      "Class: 4               - Correct: 964/982 (98.17%)\n",
      "Class: 5               - Correct: 880/892 (98.65%)\n",
      "Class: 6               - Correct: 915/958 (95.51%)\n",
      "Class: 7               - Correct: 983/1028 (95.62%)\n",
      "Class: 8               - Correct: 930/974 (95.48%)\n",
      "Class: 9               - Correct: 954/1009 (94.55%)\n",
      "\n",
      "\n",
      "Epoch [5/100], Step [100/938], Loss: 0.1627, Accuracy: 95.41%\n",
      "Epoch [5/100], Step [200/938], Loss: 0.1593, Accuracy: 95.50%\n",
      "Epoch [5/100], Step [300/938], Loss: 0.1570, Accuracy: 95.63%\n",
      "Epoch [5/100], Step [400/938], Loss: 0.1557, Accuracy: 95.65%\n",
      "Epoch [5/100], Step [500/938], Loss: 0.1546, Accuracy: 95.70%\n",
      "Epoch [5/100], Step [600/938], Loss: 0.1575, Accuracy: 95.62%\n",
      "Epoch [5/100], Step [700/938], Loss: 0.1560, Accuracy: 95.66%\n",
      "Epoch [5/100], Step [800/938], Loss: 0.1556, Accuracy: 95.68%\n",
      "Epoch [5/100], Step [900/938], Loss: 0.1551, Accuracy: 95.70%\n",
      "Epoch [5/100] completed in 9.84 seconds. Average Loss: 0.1546, Accuracy: 95.72%\n",
      "Test Loss: 0.0903, Test Accuracy: 97.41%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1125/1135 (99.12%)\n",
      "Class: 2               - Correct: 1013/1032 (98.16%)\n",
      "Class: 3               - Correct: 981/1010 (97.13%)\n",
      "Class: 4               - Correct: 956/982 (97.35%)\n",
      "Class: 5               - Correct: 875/892 (98.09%)\n",
      "Class: 6               - Correct: 933/958 (97.39%)\n",
      "Class: 7               - Correct: 1000/1028 (97.28%)\n",
      "Class: 8               - Correct: 928/974 (95.28%)\n",
      "Class: 9               - Correct: 957/1009 (94.85%)\n",
      "\n",
      "\n",
      "Epoch [6/100], Step [100/938], Loss: 0.1357, Accuracy: 96.34%\n",
      "Epoch [6/100], Step [200/938], Loss: 0.1323, Accuracy: 96.34%\n",
      "Epoch [6/100], Step [300/938], Loss: 0.1378, Accuracy: 96.14%\n",
      "Epoch [6/100], Step [400/938], Loss: 0.1397, Accuracy: 96.17%\n",
      "Epoch [6/100], Step [500/938], Loss: 0.1356, Accuracy: 96.24%\n",
      "Epoch [6/100], Step [600/938], Loss: 0.1369, Accuracy: 96.21%\n",
      "Epoch [6/100], Step [700/938], Loss: 0.1379, Accuracy: 96.19%\n",
      "Epoch [6/100], Step [800/938], Loss: 0.1387, Accuracy: 96.14%\n",
      "Epoch [6/100], Step [900/938], Loss: 0.1413, Accuracy: 96.14%\n",
      "Epoch [6/100] completed in 9.83 seconds. Average Loss: 0.1424, Accuracy: 96.09%\n",
      "Test Loss: 0.0893, Test Accuracy: 97.51%\n",
      "\n",
      "Class: 0               - Correct: 970/980 (98.98%)\n",
      "Class: 1               - Correct: 1127/1135 (99.30%)\n",
      "Class: 2               - Correct: 1006/1032 (97.48%)\n",
      "Class: 3               - Correct: 979/1010 (96.93%)\n",
      "Class: 4               - Correct: 964/982 (98.17%)\n",
      "Class: 5               - Correct: 872/892 (97.76%)\n",
      "Class: 6               - Correct: 946/958 (98.75%)\n",
      "Class: 7               - Correct: 999/1028 (97.18%)\n",
      "Class: 8               - Correct: 927/974 (95.17%)\n",
      "Class: 9               - Correct: 961/1009 (95.24%)\n",
      "\n",
      "\n",
      "Epoch [7/100], Step [100/938], Loss: 0.1295, Accuracy: 96.56%\n",
      "Epoch [7/100], Step [200/938], Loss: 0.1314, Accuracy: 96.35%\n",
      "Epoch [7/100], Step [300/938], Loss: 0.1329, Accuracy: 96.41%\n",
      "Epoch [7/100], Step [400/938], Loss: 0.1335, Accuracy: 96.34%\n",
      "Epoch [7/100], Step [500/938], Loss: 0.1344, Accuracy: 96.26%\n",
      "Epoch [7/100], Step [600/938], Loss: 0.1366, Accuracy: 96.22%\n",
      "Epoch [7/100], Step [700/938], Loss: 0.1386, Accuracy: 96.17%\n",
      "Epoch [7/100], Step [800/938], Loss: 0.1379, Accuracy: 96.21%\n",
      "Epoch [7/100], Step [900/938], Loss: 0.1377, Accuracy: 96.20%\n",
      "Epoch [7/100] completed in 9.85 seconds. Average Loss: 0.1378, Accuracy: 96.19%\n",
      "Test Loss: 0.0872, Test Accuracy: 97.52%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1129/1135 (99.47%)\n",
      "Class: 2               - Correct: 1013/1032 (98.16%)\n",
      "Class: 3               - Correct: 977/1010 (96.73%)\n",
      "Class: 4               - Correct: 951/982 (96.84%)\n",
      "Class: 5               - Correct: 870/892 (97.53%)\n",
      "Class: 6               - Correct: 939/958 (98.02%)\n",
      "Class: 7               - Correct: 997/1028 (96.98%)\n",
      "Class: 8               - Correct: 937/974 (96.20%)\n",
      "Class: 9               - Correct: 967/1009 (95.84%)\n",
      "\n",
      "\n",
      "Epoch [8/100], Step [100/938], Loss: 0.1308, Accuracy: 96.38%\n",
      "Epoch [8/100], Step [200/938], Loss: 0.1325, Accuracy: 96.17%\n",
      "Epoch [8/100], Step [300/938], Loss: 0.1316, Accuracy: 96.24%\n",
      "Epoch [8/100], Step [400/938], Loss: 0.1319, Accuracy: 96.29%\n",
      "Epoch [8/100], Step [500/938], Loss: 0.1295, Accuracy: 96.37%\n",
      "Epoch [8/100], Step [600/938], Loss: 0.1301, Accuracy: 96.37%\n",
      "Epoch [8/100], Step [700/938], Loss: 0.1291, Accuracy: 96.41%\n",
      "Epoch [8/100], Step [800/938], Loss: 0.1290, Accuracy: 96.41%\n",
      "Epoch [8/100], Step [900/938], Loss: 0.1289, Accuracy: 96.44%\n",
      "Epoch [8/100] completed in 9.77 seconds. Average Loss: 0.1295, Accuracy: 96.42%\n",
      "Test Loss: 0.0827, Test Accuracy: 97.70%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1125/1135 (99.12%)\n",
      "Class: 2               - Correct: 998/1032 (96.71%)\n",
      "Class: 3               - Correct: 988/1010 (97.82%)\n",
      "Class: 4               - Correct: 956/982 (97.35%)\n",
      "Class: 5               - Correct: 869/892 (97.42%)\n",
      "Class: 6               - Correct: 934/958 (97.49%)\n",
      "Class: 7               - Correct: 1003/1028 (97.57%)\n",
      "Class: 8               - Correct: 948/974 (97.33%)\n",
      "Class: 9               - Correct: 976/1009 (96.73%)\n",
      "\n",
      "\n",
      "Epoch [9/100], Step [100/938], Loss: 0.1267, Accuracy: 96.44%\n",
      "Epoch [9/100], Step [200/938], Loss: 0.1227, Accuracy: 96.58%\n",
      "Epoch [9/100], Step [300/938], Loss: 0.1193, Accuracy: 96.58%\n",
      "Epoch [9/100], Step [400/938], Loss: 0.1190, Accuracy: 96.62%\n",
      "Epoch [9/100], Step [500/938], Loss: 0.1196, Accuracy: 96.62%\n",
      "Epoch [9/100], Step [600/938], Loss: 0.1238, Accuracy: 96.54%\n",
      "Epoch [9/100], Step [700/938], Loss: 0.1249, Accuracy: 96.51%\n",
      "Epoch [9/100], Step [800/938], Loss: 0.1271, Accuracy: 96.45%\n",
      "Epoch [9/100], Step [900/938], Loss: 0.1262, Accuracy: 96.45%\n",
      "Epoch [9/100] completed in 9.82 seconds. Average Loss: 0.1264, Accuracy: 96.46%\n",
      "Test Loss: 0.0833, Test Accuracy: 97.74%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1129/1135 (99.47%)\n",
      "Class: 2               - Correct: 1011/1032 (97.97%)\n",
      "Class: 3               - Correct: 993/1010 (98.32%)\n",
      "Class: 4               - Correct: 962/982 (97.96%)\n",
      "Class: 5               - Correct: 872/892 (97.76%)\n",
      "Class: 6               - Correct: 939/958 (98.02%)\n",
      "Class: 7               - Correct: 1004/1028 (97.67%)\n",
      "Class: 8               - Correct: 937/974 (96.20%)\n",
      "Class: 9               - Correct: 955/1009 (94.65%)\n",
      "\n",
      "\n",
      "Epoch [10/100], Step [100/938], Loss: 0.1123, Accuracy: 96.98%\n",
      "Epoch [10/100], Step [200/938], Loss: 0.1175, Accuracy: 96.79%\n",
      "Epoch [10/100], Step [300/938], Loss: 0.1172, Accuracy: 96.82%\n",
      "Epoch [10/100], Step [400/938], Loss: 0.1152, Accuracy: 96.86%\n",
      "Epoch [10/100], Step [500/938], Loss: 0.1163, Accuracy: 96.84%\n",
      "Epoch [10/100], Step [600/938], Loss: 0.1153, Accuracy: 96.87%\n",
      "Epoch [10/100], Step [700/938], Loss: 0.1156, Accuracy: 96.81%\n",
      "Epoch [10/100], Step [800/938], Loss: 0.1157, Accuracy: 96.81%\n",
      "Epoch [10/100], Step [900/938], Loss: 0.1164, Accuracy: 96.80%\n",
      "Epoch [10/100] completed in 9.82 seconds. Average Loss: 0.1172, Accuracy: 96.79%\n",
      "Test Loss: 0.0821, Test Accuracy: 97.83%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1128/1135 (99.38%)\n",
      "Class: 2               - Correct: 1017/1032 (98.55%)\n",
      "Class: 3               - Correct: 972/1010 (96.24%)\n",
      "Class: 4               - Correct: 961/982 (97.86%)\n",
      "Class: 5               - Correct: 867/892 (97.20%)\n",
      "Class: 6               - Correct: 946/958 (98.75%)\n",
      "Class: 7               - Correct: 1006/1028 (97.86%)\n",
      "Class: 8               - Correct: 950/974 (97.54%)\n",
      "Class: 9               - Correct: 964/1009 (95.54%)\n",
      "\n",
      "\n",
      "Epoch [11/100], Step [100/938], Loss: 0.1143, Accuracy: 96.91%\n",
      "Epoch [11/100], Step [200/938], Loss: 0.1191, Accuracy: 96.70%\n",
      "Epoch [11/100], Step [300/938], Loss: 0.1180, Accuracy: 96.74%\n",
      "Epoch [11/100], Step [400/938], Loss: 0.1162, Accuracy: 96.74%\n",
      "Epoch [11/100], Step [500/938], Loss: 0.1173, Accuracy: 96.78%\n",
      "Epoch [11/100], Step [600/938], Loss: 0.1150, Accuracy: 96.84%\n",
      "Epoch [11/100], Step [700/938], Loss: 0.1146, Accuracy: 96.89%\n",
      "Epoch [11/100], Step [800/938], Loss: 0.1144, Accuracy: 96.87%\n",
      "Epoch [11/100], Step [900/938], Loss: 0.1156, Accuracy: 96.86%\n",
      "Epoch [11/100] completed in 9.54 seconds. Average Loss: 0.1151, Accuracy: 96.88%\n",
      "Test Loss: 0.0771, Test Accuracy: 97.98%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1014/1032 (98.26%)\n",
      "Class: 3               - Correct: 994/1010 (98.42%)\n",
      "Class: 4               - Correct: 967/982 (98.47%)\n",
      "Class: 5               - Correct: 869/892 (97.42%)\n",
      "Class: 6               - Correct: 941/958 (98.23%)\n",
      "Class: 7               - Correct: 1000/1028 (97.28%)\n",
      "Class: 8               - Correct: 947/974 (97.23%)\n",
      "Class: 9               - Correct: 968/1009 (95.94%)\n",
      "\n",
      "\n",
      "Epoch [12/100], Step [100/938], Loss: 0.1273, Accuracy: 96.78%\n",
      "Epoch [12/100], Step [200/938], Loss: 0.1135, Accuracy: 96.99%\n",
      "Epoch [12/100], Step [300/938], Loss: 0.1111, Accuracy: 96.95%\n",
      "Epoch [12/100], Step [400/938], Loss: 0.1134, Accuracy: 96.86%\n",
      "Epoch [12/100], Step [500/938], Loss: 0.1102, Accuracy: 96.95%\n",
      "Epoch [12/100], Step [600/938], Loss: 0.1101, Accuracy: 96.95%\n",
      "Epoch [12/100], Step [700/938], Loss: 0.1122, Accuracy: 96.88%\n",
      "Epoch [12/100], Step [800/938], Loss: 0.1134, Accuracy: 96.84%\n",
      "Epoch [12/100], Step [900/938], Loss: 0.1136, Accuracy: 96.83%\n",
      "Epoch [12/100] completed in 9.59 seconds. Average Loss: 0.1152, Accuracy: 96.78%\n",
      "Test Loss: 0.0800, Test Accuracy: 97.86%\n",
      "\n",
      "Class: 0               - Correct: 974/980 (99.39%)\n",
      "Class: 1               - Correct: 1127/1135 (99.30%)\n",
      "Class: 2               - Correct: 1003/1032 (97.19%)\n",
      "Class: 3               - Correct: 991/1010 (98.12%)\n",
      "Class: 4               - Correct: 959/982 (97.66%)\n",
      "Class: 5               - Correct: 871/892 (97.65%)\n",
      "Class: 6               - Correct: 943/958 (98.43%)\n",
      "Class: 7               - Correct: 999/1028 (97.18%)\n",
      "Class: 8               - Correct: 949/974 (97.43%)\n",
      "Class: 9               - Correct: 970/1009 (96.13%)\n",
      "\n",
      "\n",
      "Epoch [13/100], Step [100/938], Loss: 0.0934, Accuracy: 97.22%\n",
      "Epoch [13/100], Step [200/938], Loss: 0.1033, Accuracy: 97.08%\n",
      "Epoch [13/100], Step [300/938], Loss: 0.1075, Accuracy: 97.08%\n",
      "Epoch [13/100], Step [400/938], Loss: 0.1072, Accuracy: 97.12%\n",
      "Epoch [13/100], Step [500/938], Loss: 0.1113, Accuracy: 97.03%\n",
      "Epoch [13/100], Step [600/938], Loss: 0.1098, Accuracy: 97.05%\n",
      "Epoch [13/100], Step [700/938], Loss: 0.1108, Accuracy: 97.06%\n",
      "Epoch [13/100], Step [800/938], Loss: 0.1121, Accuracy: 97.04%\n",
      "Epoch [13/100], Step [900/938], Loss: 0.1128, Accuracy: 97.03%\n",
      "Epoch [13/100] completed in 9.51 seconds. Average Loss: 0.1124, Accuracy: 97.02%\n",
      "Test Loss: 0.0757, Test Accuracy: 98.03%\n",
      "\n",
      "Class: 0               - Correct: 974/980 (99.39%)\n",
      "Class: 1               - Correct: 1128/1135 (99.38%)\n",
      "Class: 2               - Correct: 1011/1032 (97.97%)\n",
      "Class: 3               - Correct: 987/1010 (97.72%)\n",
      "Class: 4               - Correct: 957/982 (97.45%)\n",
      "Class: 5               - Correct: 879/892 (98.54%)\n",
      "Class: 6               - Correct: 944/958 (98.54%)\n",
      "Class: 7               - Correct: 1003/1028 (97.57%)\n",
      "Class: 8               - Correct: 939/974 (96.41%)\n",
      "Class: 9               - Correct: 981/1009 (97.22%)\n",
      "\n",
      "\n",
      "Epoch [14/100], Step [100/938], Loss: 0.1005, Accuracy: 97.20%\n",
      "Epoch [14/100], Step [200/938], Loss: 0.0918, Accuracy: 97.37%\n",
      "Epoch [14/100], Step [300/938], Loss: 0.0953, Accuracy: 97.32%\n",
      "Epoch [14/100], Step [400/938], Loss: 0.0977, Accuracy: 97.29%\n",
      "Epoch [14/100], Step [500/938], Loss: 0.0983, Accuracy: 97.25%\n",
      "Epoch [14/100], Step [600/938], Loss: 0.1012, Accuracy: 97.20%\n",
      "Epoch [14/100], Step [700/938], Loss: 0.1030, Accuracy: 97.15%\n",
      "Epoch [14/100], Step [800/938], Loss: 0.1059, Accuracy: 97.07%\n",
      "Epoch [14/100], Step [900/938], Loss: 0.1092, Accuracy: 97.01%\n",
      "Epoch [14/100] completed in 9.59 seconds. Average Loss: 0.1109, Accuracy: 96.97%\n",
      "Test Loss: 0.0843, Test Accuracy: 97.86%\n",
      "\n",
      "Class: 0               - Correct: 975/980 (99.49%)\n",
      "Class: 1               - Correct: 1127/1135 (99.30%)\n",
      "Class: 2               - Correct: 1012/1032 (98.06%)\n",
      "Class: 3               - Correct: 995/1010 (98.51%)\n",
      "Class: 4               - Correct: 963/982 (98.07%)\n",
      "Class: 5               - Correct: 873/892 (97.87%)\n",
      "Class: 6               - Correct: 940/958 (98.12%)\n",
      "Class: 7               - Correct: 1002/1028 (97.47%)\n",
      "Class: 8               - Correct: 933/974 (95.79%)\n",
      "Class: 9               - Correct: 966/1009 (95.74%)\n",
      "\n",
      "\n",
      "Epoch [15/100], Step [100/938], Loss: 0.0808, Accuracy: 97.66%\n",
      "Epoch [15/100], Step [200/938], Loss: 0.0889, Accuracy: 97.52%\n",
      "Epoch [15/100], Step [300/938], Loss: 0.0940, Accuracy: 97.43%\n",
      "Epoch [15/100], Step [400/938], Loss: 0.1020, Accuracy: 97.28%\n",
      "Epoch [15/100], Step [500/938], Loss: 0.1038, Accuracy: 97.20%\n",
      "Epoch [15/100], Step [600/938], Loss: 0.1036, Accuracy: 97.15%\n",
      "Epoch [15/100], Step [700/938], Loss: 0.1080, Accuracy: 97.06%\n",
      "Epoch [15/100], Step [800/938], Loss: 0.1084, Accuracy: 97.07%\n",
      "Epoch [15/100], Step [900/938], Loss: 0.1091, Accuracy: 97.04%\n",
      "Epoch [15/100] completed in 9.51 seconds. Average Loss: 0.1084, Accuracy: 97.04%\n",
      "Test Loss: 0.0768, Test Accuracy: 98.02%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1124/1135 (99.03%)\n",
      "Class: 2               - Correct: 1017/1032 (98.55%)\n",
      "Class: 3               - Correct: 991/1010 (98.12%)\n",
      "Class: 4               - Correct: 968/982 (98.57%)\n",
      "Class: 5               - Correct: 865/892 (96.97%)\n",
      "Class: 6               - Correct: 938/958 (97.91%)\n",
      "Class: 7               - Correct: 1000/1028 (97.28%)\n",
      "Class: 8               - Correct: 952/974 (97.74%)\n",
      "Class: 9               - Correct: 974/1009 (96.53%)\n",
      "\n",
      "\n",
      "Epoch [16/100], Step [100/938], Loss: 0.0929, Accuracy: 97.42%\n",
      "Epoch [16/100], Step [200/938], Loss: 0.1010, Accuracy: 97.34%\n",
      "Epoch [16/100], Step [300/938], Loss: 0.1028, Accuracy: 97.25%\n",
      "Epoch [16/100], Step [400/938], Loss: 0.0998, Accuracy: 97.30%\n",
      "Epoch [16/100], Step [500/938], Loss: 0.0986, Accuracy: 97.32%\n",
      "Epoch [16/100], Step [600/938], Loss: 0.1021, Accuracy: 97.26%\n",
      "Epoch [16/100], Step [700/938], Loss: 0.1040, Accuracy: 97.20%\n",
      "Epoch [16/100], Step [800/938], Loss: 0.1030, Accuracy: 97.18%\n",
      "Epoch [16/100], Step [900/938], Loss: 0.1021, Accuracy: 97.23%\n",
      "Epoch [16/100] completed in 9.55 seconds. Average Loss: 0.1031, Accuracy: 97.22%\n",
      "Test Loss: 0.0794, Test Accuracy: 98.00%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1127/1135 (99.30%)\n",
      "Class: 2               - Correct: 1015/1032 (98.35%)\n",
      "Class: 3               - Correct: 989/1010 (97.92%)\n",
      "Class: 4               - Correct: 970/982 (98.78%)\n",
      "Class: 5               - Correct: 858/892 (96.19%)\n",
      "Class: 6               - Correct: 941/958 (98.23%)\n",
      "Class: 7               - Correct: 1014/1028 (98.64%)\n",
      "Class: 8               - Correct: 944/974 (96.92%)\n",
      "Class: 9               - Correct: 969/1009 (96.04%)\n",
      "\n",
      "\n",
      "Epoch [17/100], Step [100/938], Loss: 0.0964, Accuracy: 97.34%\n",
      "Epoch [17/100], Step [200/938], Loss: 0.0962, Accuracy: 97.30%\n",
      "Epoch [17/100], Step [300/938], Loss: 0.0976, Accuracy: 97.28%\n",
      "Epoch [17/100], Step [400/938], Loss: 0.0962, Accuracy: 97.32%\n",
      "Epoch [17/100], Step [500/938], Loss: 0.0959, Accuracy: 97.29%\n",
      "Epoch [17/100], Step [600/938], Loss: 0.0978, Accuracy: 97.25%\n",
      "Epoch [17/100], Step [700/938], Loss: 0.0995, Accuracy: 97.21%\n",
      "Epoch [17/100], Step [800/938], Loss: 0.1003, Accuracy: 97.19%\n",
      "Epoch [17/100], Step [900/938], Loss: 0.1019, Accuracy: 97.17%\n",
      "Epoch [17/100] completed in 9.59 seconds. Average Loss: 0.1015, Accuracy: 97.18%\n",
      "Test Loss: 0.0754, Test Accuracy: 97.97%\n",
      "\n",
      "Class: 0               - Correct: 969/980 (98.88%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1009/1032 (97.77%)\n",
      "Class: 3               - Correct: 994/1010 (98.42%)\n",
      "Class: 4               - Correct: 955/982 (97.25%)\n",
      "Class: 5               - Correct: 874/892 (97.98%)\n",
      "Class: 6               - Correct: 943/958 (98.43%)\n",
      "Class: 7               - Correct: 995/1028 (96.79%)\n",
      "Class: 8               - Correct: 945/974 (97.02%)\n",
      "Class: 9               - Correct: 987/1009 (97.82%)\n",
      "\n",
      "\n",
      "Epoch [18/100], Step [100/938], Loss: 0.1016, Accuracy: 97.59%\n",
      "Epoch [18/100], Step [200/938], Loss: 0.0953, Accuracy: 97.56%\n",
      "Epoch [18/100], Step [300/938], Loss: 0.0951, Accuracy: 97.43%\n",
      "Epoch [18/100], Step [400/938], Loss: 0.0977, Accuracy: 97.39%\n",
      "Epoch [18/100], Step [500/938], Loss: 0.0987, Accuracy: 97.38%\n",
      "Epoch [18/100], Step [600/938], Loss: 0.0997, Accuracy: 97.32%\n",
      "Epoch [18/100], Step [700/938], Loss: 0.0989, Accuracy: 97.32%\n",
      "Epoch [18/100], Step [800/938], Loss: 0.0980, Accuracy: 97.35%\n",
      "Epoch [18/100], Step [900/938], Loss: 0.1013, Accuracy: 97.27%\n",
      "Epoch [18/100] completed in 9.75 seconds. Average Loss: 0.1011, Accuracy: 97.28%\n",
      "Test Loss: 0.0751, Test Accuracy: 98.06%\n",
      "\n",
      "Class: 0               - Correct: 975/980 (99.49%)\n",
      "Class: 1               - Correct: 1124/1135 (99.03%)\n",
      "Class: 2               - Correct: 1014/1032 (98.26%)\n",
      "Class: 3               - Correct: 994/1010 (98.42%)\n",
      "Class: 4               - Correct: 950/982 (96.74%)\n",
      "Class: 5               - Correct: 871/892 (97.65%)\n",
      "Class: 6               - Correct: 940/958 (98.12%)\n",
      "Class: 7               - Correct: 1004/1028 (97.67%)\n",
      "Class: 8               - Correct: 949/974 (97.43%)\n",
      "Class: 9               - Correct: 985/1009 (97.62%)\n",
      "\n",
      "\n",
      "Epoch [19/100], Step [100/938], Loss: 0.1059, Accuracy: 97.08%\n",
      "Epoch [19/100], Step [200/938], Loss: 0.0994, Accuracy: 97.23%\n",
      "Epoch [19/100], Step [300/938], Loss: 0.0974, Accuracy: 97.29%\n",
      "Epoch [19/100], Step [400/938], Loss: 0.0959, Accuracy: 97.32%\n",
      "Epoch [19/100], Step [500/938], Loss: 0.0981, Accuracy: 97.32%\n",
      "Epoch [19/100], Step [600/938], Loss: 0.0971, Accuracy: 97.30%\n",
      "Epoch [19/100], Step [700/938], Loss: 0.0983, Accuracy: 97.29%\n",
      "Epoch [19/100], Step [800/938], Loss: 0.0975, Accuracy: 97.32%\n",
      "Epoch [19/100], Step [900/938], Loss: 0.0971, Accuracy: 97.35%\n",
      "Epoch [19/100] completed in 9.79 seconds. Average Loss: 0.0973, Accuracy: 97.35%\n",
      "Test Loss: 0.0852, Test Accuracy: 97.94%\n",
      "\n",
      "Class: 0               - Correct: 969/980 (98.88%)\n",
      "Class: 1               - Correct: 1122/1135 (98.85%)\n",
      "Class: 2               - Correct: 1010/1032 (97.87%)\n",
      "Class: 3               - Correct: 994/1010 (98.42%)\n",
      "Class: 4               - Correct: 963/982 (98.07%)\n",
      "Class: 5               - Correct: 864/892 (96.86%)\n",
      "Class: 6               - Correct: 948/958 (98.96%)\n",
      "Class: 7               - Correct: 1015/1028 (98.74%)\n",
      "Class: 8               - Correct: 943/974 (96.82%)\n",
      "Class: 9               - Correct: 966/1009 (95.74%)\n",
      "\n",
      "\n",
      "Epoch [20/100], Step [100/938], Loss: 0.0921, Accuracy: 97.58%\n",
      "Epoch [20/100], Step [200/938], Loss: 0.0932, Accuracy: 97.49%\n",
      "Epoch [20/100], Step [300/938], Loss: 0.0920, Accuracy: 97.48%\n",
      "Epoch [20/100], Step [400/938], Loss: 0.0921, Accuracy: 97.49%\n",
      "Epoch [20/100], Step [500/938], Loss: 0.0908, Accuracy: 97.49%\n",
      "Epoch [20/100], Step [600/938], Loss: 0.0933, Accuracy: 97.44%\n",
      "Epoch [20/100], Step [700/938], Loss: 0.0935, Accuracy: 97.44%\n",
      "Epoch [20/100], Step [800/938], Loss: 0.0948, Accuracy: 97.41%\n",
      "Epoch [20/100], Step [900/938], Loss: 0.0944, Accuracy: 97.42%\n",
      "Epoch [20/100] completed in 9.80 seconds. Average Loss: 0.0946, Accuracy: 97.42%\n",
      "Test Loss: 0.0770, Test Accuracy: 98.12%\n",
      "\n",
      "Class: 0               - Correct: 971/980 (99.08%)\n",
      "Class: 1               - Correct: 1124/1135 (99.03%)\n",
      "Class: 2               - Correct: 1013/1032 (98.16%)\n",
      "Class: 3               - Correct: 991/1010 (98.12%)\n",
      "Class: 4               - Correct: 955/982 (97.25%)\n",
      "Class: 5               - Correct: 880/892 (98.65%)\n",
      "Class: 6               - Correct: 943/958 (98.43%)\n",
      "Class: 7               - Correct: 1012/1028 (98.44%)\n",
      "Class: 8               - Correct: 950/974 (97.54%)\n",
      "Class: 9               - Correct: 973/1009 (96.43%)\n",
      "\n",
      "\n",
      "Epoch [21/100], Step [100/938], Loss: 0.0962, Accuracy: 97.45%\n",
      "Epoch [21/100], Step [200/938], Loss: 0.0992, Accuracy: 97.31%\n",
      "Epoch [21/100], Step [300/938], Loss: 0.1018, Accuracy: 97.32%\n",
      "Epoch [21/100], Step [400/938], Loss: 0.0970, Accuracy: 97.45%\n",
      "Epoch [21/100], Step [500/938], Loss: 0.0939, Accuracy: 97.50%\n",
      "Epoch [21/100], Step [600/938], Loss: 0.0948, Accuracy: 97.48%\n",
      "Epoch [21/100], Step [700/938], Loss: 0.0933, Accuracy: 97.49%\n",
      "Epoch [21/100], Step [800/938], Loss: 0.0943, Accuracy: 97.47%\n",
      "Epoch [21/100], Step [900/938], Loss: 0.0945, Accuracy: 97.45%\n",
      "Epoch [21/100] completed in 9.81 seconds. Average Loss: 0.0943, Accuracy: 97.45%\n",
      "Test Loss: 0.0808, Test Accuracy: 98.03%\n",
      "\n",
      "Class: 0               - Correct: 969/980 (98.88%)\n",
      "Class: 1               - Correct: 1121/1135 (98.77%)\n",
      "Class: 2               - Correct: 1017/1032 (98.55%)\n",
      "Class: 3               - Correct: 984/1010 (97.43%)\n",
      "Class: 4               - Correct: 967/982 (98.47%)\n",
      "Class: 5               - Correct: 881/892 (98.77%)\n",
      "Class: 6               - Correct: 943/958 (98.43%)\n",
      "Class: 7               - Correct: 1006/1028 (97.86%)\n",
      "Class: 8               - Correct: 945/974 (97.02%)\n",
      "Class: 9               - Correct: 970/1009 (96.13%)\n",
      "\n",
      "\n",
      "Epoch [22/100], Step [100/938], Loss: 0.0936, Accuracy: 97.38%\n",
      "Epoch [22/100], Step [200/938], Loss: 0.0933, Accuracy: 97.53%\n",
      "Epoch [22/100], Step [300/938], Loss: 0.0922, Accuracy: 97.53%\n",
      "Epoch [22/100], Step [400/938], Loss: 0.0926, Accuracy: 97.52%\n",
      "Epoch [22/100], Step [500/938], Loss: 0.0940, Accuracy: 97.53%\n",
      "Epoch [22/100], Step [600/938], Loss: 0.0926, Accuracy: 97.51%\n",
      "Epoch [22/100], Step [700/938], Loss: 0.0922, Accuracy: 97.52%\n",
      "Epoch [22/100], Step [800/938], Loss: 0.0917, Accuracy: 97.50%\n",
      "Epoch [22/100], Step [900/938], Loss: 0.0934, Accuracy: 97.48%\n",
      "Epoch [22/100] completed in 9.77 seconds. Average Loss: 0.0934, Accuracy: 97.49%\n",
      "Test Loss: 0.0786, Test Accuracy: 98.11%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1008/1032 (97.67%)\n",
      "Class: 3               - Correct: 998/1010 (98.81%)\n",
      "Class: 4               - Correct: 962/982 (97.96%)\n",
      "Class: 5               - Correct: 866/892 (97.09%)\n",
      "Class: 6               - Correct: 941/958 (98.23%)\n",
      "Class: 7               - Correct: 1012/1028 (98.44%)\n",
      "Class: 8               - Correct: 945/974 (97.02%)\n",
      "Class: 9               - Correct: 980/1009 (97.13%)\n",
      "\n",
      "\n",
      "Epoch [23/100], Step [100/938], Loss: 0.1027, Accuracy: 97.38%\n",
      "Epoch [23/100], Step [200/938], Loss: 0.0935, Accuracy: 97.55%\n",
      "Epoch [23/100], Step [300/938], Loss: 0.0905, Accuracy: 97.59%\n",
      "Epoch [23/100], Step [400/938], Loss: 0.0871, Accuracy: 97.64%\n",
      "Epoch [23/100], Step [500/938], Loss: 0.0889, Accuracy: 97.61%\n",
      "Epoch [23/100], Step [600/938], Loss: 0.0884, Accuracy: 97.59%\n",
      "Epoch [23/100], Step [700/938], Loss: 0.0907, Accuracy: 97.56%\n",
      "Epoch [23/100], Step [800/938], Loss: 0.0901, Accuracy: 97.57%\n",
      "Epoch [23/100], Step [900/938], Loss: 0.0893, Accuracy: 97.59%\n",
      "Epoch [23/100] completed in 9.79 seconds. Average Loss: 0.0891, Accuracy: 97.59%\n",
      "Test Loss: 0.0849, Test Accuracy: 98.03%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1128/1135 (99.38%)\n",
      "Class: 2               - Correct: 1008/1032 (97.67%)\n",
      "Class: 3               - Correct: 994/1010 (98.42%)\n",
      "Class: 4               - Correct: 959/982 (97.66%)\n",
      "Class: 5               - Correct: 858/892 (96.19%)\n",
      "Class: 6               - Correct: 941/958 (98.23%)\n",
      "Class: 7               - Correct: 1007/1028 (97.96%)\n",
      "Class: 8               - Correct: 955/974 (98.05%)\n",
      "Class: 9               - Correct: 980/1009 (97.13%)\n",
      "\n",
      "\n",
      "Epoch [24/100], Step [100/938], Loss: 0.0968, Accuracy: 97.38%\n",
      "Epoch [24/100], Step [200/938], Loss: 0.0982, Accuracy: 97.46%\n",
      "Epoch [24/100], Step [300/938], Loss: 0.0950, Accuracy: 97.53%\n",
      "Epoch [24/100], Step [400/938], Loss: 0.0908, Accuracy: 97.62%\n",
      "Epoch [24/100], Step [500/938], Loss: 0.0904, Accuracy: 97.62%\n",
      "Epoch [24/100], Step [600/938], Loss: 0.0912, Accuracy: 97.61%\n",
      "Epoch [24/100], Step [700/938], Loss: 0.0909, Accuracy: 97.59%\n",
      "Epoch [24/100], Step [800/938], Loss: 0.0894, Accuracy: 97.63%\n",
      "Epoch [24/100], Step [900/938], Loss: 0.0876, Accuracy: 97.67%\n",
      "Epoch [24/100] completed in 9.71 seconds. Average Loss: 0.0885, Accuracy: 97.66%\n",
      "Test Loss: 0.0858, Test Accuracy: 98.14%\n",
      "\n",
      "Class: 0               - Correct: 970/980 (98.98%)\n",
      "Class: 1               - Correct: 1123/1135 (98.94%)\n",
      "Class: 2               - Correct: 1020/1032 (98.84%)\n",
      "Class: 3               - Correct: 996/1010 (98.61%)\n",
      "Class: 4               - Correct: 966/982 (98.37%)\n",
      "Class: 5               - Correct: 865/892 (96.97%)\n",
      "Class: 6               - Correct: 942/958 (98.33%)\n",
      "Class: 7               - Correct: 1008/1028 (98.05%)\n",
      "Class: 8               - Correct: 954/974 (97.95%)\n",
      "Class: 9               - Correct: 970/1009 (96.13%)\n",
      "\n",
      "\n",
      "Epoch [25/100], Step [100/938], Loss: 0.0838, Accuracy: 97.80%\n",
      "Epoch [25/100], Step [200/938], Loss: 0.0955, Accuracy: 97.59%\n",
      "Epoch [25/100], Step [300/938], Loss: 0.0920, Accuracy: 97.62%\n",
      "Epoch [25/100], Step [400/938], Loss: 0.0915, Accuracy: 97.61%\n",
      "Epoch [25/100], Step [500/938], Loss: 0.0901, Accuracy: 97.60%\n",
      "Epoch [25/100], Step [600/938], Loss: 0.0884, Accuracy: 97.62%\n",
      "Epoch [25/100], Step [700/938], Loss: 0.0895, Accuracy: 97.61%\n",
      "Epoch [25/100], Step [800/938], Loss: 0.0903, Accuracy: 97.59%\n",
      "Epoch [25/100], Step [900/938], Loss: 0.0900, Accuracy: 97.58%\n",
      "Epoch [25/100] completed in 9.67 seconds. Average Loss: 0.0902, Accuracy: 97.57%\n",
      "Test Loss: 0.0841, Test Accuracy: 98.22%\n",
      "\n",
      "Class: 0               - Correct: 974/980 (99.39%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1016/1032 (98.45%)\n",
      "Class: 3               - Correct: 988/1010 (97.82%)\n",
      "Class: 4               - Correct: 964/982 (98.17%)\n",
      "Class: 5               - Correct: 878/892 (98.43%)\n",
      "Class: 6               - Correct: 941/958 (98.23%)\n",
      "Class: 7               - Correct: 1005/1028 (97.76%)\n",
      "Class: 8               - Correct: 944/974 (96.92%)\n",
      "Class: 9               - Correct: 986/1009 (97.72%)\n",
      "\n",
      "\n",
      "Epoch [26/100], Step [100/938], Loss: 0.0810, Accuracy: 97.73%\n",
      "Epoch [26/100], Step [200/938], Loss: 0.0818, Accuracy: 97.80%\n",
      "Epoch [26/100], Step [300/938], Loss: 0.0833, Accuracy: 97.81%\n",
      "Epoch [26/100], Step [400/938], Loss: 0.0824, Accuracy: 97.82%\n",
      "Epoch [26/100], Step [500/938], Loss: 0.0845, Accuracy: 97.73%\n",
      "Epoch [26/100], Step [600/938], Loss: 0.0861, Accuracy: 97.68%\n",
      "Epoch [26/100], Step [700/938], Loss: 0.0847, Accuracy: 97.70%\n",
      "Epoch [26/100], Step [800/938], Loss: 0.0838, Accuracy: 97.71%\n",
      "Epoch [26/100], Step [900/938], Loss: 0.0845, Accuracy: 97.68%\n",
      "Epoch [26/100] completed in 9.63 seconds. Average Loss: 0.0856, Accuracy: 97.66%\n",
      "Test Loss: 0.0900, Test Accuracy: 97.83%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1121/1135 (98.77%)\n",
      "Class: 2               - Correct: 1008/1032 (97.67%)\n",
      "Class: 3               - Correct: 983/1010 (97.33%)\n",
      "Class: 4               - Correct: 964/982 (98.17%)\n",
      "Class: 5               - Correct: 880/892 (98.65%)\n",
      "Class: 6               - Correct: 936/958 (97.70%)\n",
      "Class: 7               - Correct: 1005/1028 (97.76%)\n",
      "Class: 8               - Correct: 951/974 (97.64%)\n",
      "Class: 9               - Correct: 963/1009 (95.44%)\n",
      "\n",
      "\n",
      "Epoch [27/100], Step [100/938], Loss: 0.0859, Accuracy: 97.38%\n",
      "Epoch [27/100], Step [200/938], Loss: 0.0813, Accuracy: 97.70%\n",
      "Epoch [27/100], Step [300/938], Loss: 0.0802, Accuracy: 97.77%\n",
      "Epoch [27/100], Step [400/938], Loss: 0.0823, Accuracy: 97.76%\n",
      "Epoch [27/100], Step [500/938], Loss: 0.0828, Accuracy: 97.70%\n",
      "Epoch [27/100], Step [600/938], Loss: 0.0830, Accuracy: 97.69%\n",
      "Epoch [27/100], Step [700/938], Loss: 0.0831, Accuracy: 97.69%\n",
      "Epoch [27/100], Step [800/938], Loss: 0.0839, Accuracy: 97.70%\n",
      "Epoch [27/100], Step [900/938], Loss: 0.0853, Accuracy: 97.67%\n",
      "Epoch [27/100] completed in 9.60 seconds. Average Loss: 0.0861, Accuracy: 97.64%\n",
      "Test Loss: 0.0852, Test Accuracy: 98.08%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1128/1135 (99.38%)\n",
      "Class: 2               - Correct: 1002/1032 (97.09%)\n",
      "Class: 3               - Correct: 988/1010 (97.82%)\n",
      "Class: 4               - Correct: 965/982 (98.27%)\n",
      "Class: 5               - Correct: 869/892 (97.42%)\n",
      "Class: 6               - Correct: 940/958 (98.12%)\n",
      "Class: 7               - Correct: 1014/1028 (98.64%)\n",
      "Class: 8               - Correct: 956/974 (98.15%)\n",
      "Class: 9               - Correct: 974/1009 (96.53%)\n",
      "\n",
      "\n",
      "Epoch [28/100], Step [100/938], Loss: 0.0865, Accuracy: 97.92%\n",
      "Epoch [28/100], Step [200/938], Loss: 0.0854, Accuracy: 97.91%\n",
      "Epoch [28/100], Step [300/938], Loss: 0.0823, Accuracy: 97.96%\n",
      "Epoch [28/100], Step [400/938], Loss: 0.0827, Accuracy: 97.96%\n",
      "Epoch [28/100], Step [500/938], Loss: 0.0842, Accuracy: 97.86%\n",
      "Epoch [28/100], Step [600/938], Loss: 0.0854, Accuracy: 97.78%\n",
      "Epoch [28/100], Step [700/938], Loss: 0.0855, Accuracy: 97.78%\n",
      "Epoch [28/100], Step [800/938], Loss: 0.0856, Accuracy: 97.78%\n",
      "Epoch [28/100], Step [900/938], Loss: 0.0840, Accuracy: 97.81%\n",
      "Epoch [28/100] completed in 9.67 seconds. Average Loss: 0.0832, Accuracy: 97.82%\n",
      "Test Loss: 0.0917, Test Accuracy: 98.00%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1012/1032 (98.06%)\n",
      "Class: 3               - Correct: 981/1010 (97.13%)\n",
      "Class: 4               - Correct: 960/982 (97.76%)\n",
      "Class: 5               - Correct: 873/892 (97.87%)\n",
      "Class: 6               - Correct: 947/958 (98.85%)\n",
      "Class: 7               - Correct: 1007/1028 (97.96%)\n",
      "Class: 8               - Correct: 946/974 (97.13%)\n",
      "Class: 9               - Correct: 976/1009 (96.73%)\n",
      "\n",
      "\n",
      "Epoch [29/100], Step [100/938], Loss: 0.0796, Accuracy: 97.69%\n",
      "Epoch [29/100], Step [200/938], Loss: 0.0831, Accuracy: 97.68%\n",
      "Epoch [29/100], Step [300/938], Loss: 0.0782, Accuracy: 97.76%\n",
      "Epoch [29/100], Step [400/938], Loss: 0.0792, Accuracy: 97.75%\n",
      "Epoch [29/100], Step [500/938], Loss: 0.0789, Accuracy: 97.80%\n",
      "Epoch [29/100], Step [600/938], Loss: 0.0798, Accuracy: 97.78%\n",
      "Epoch [29/100], Step [700/938], Loss: 0.0808, Accuracy: 97.77%\n",
      "Epoch [29/100], Step [800/938], Loss: 0.0816, Accuracy: 97.77%\n",
      "Epoch [29/100], Step [900/938], Loss: 0.0824, Accuracy: 97.73%\n",
      "Epoch [29/100] completed in 9.60 seconds. Average Loss: 0.0821, Accuracy: 97.74%\n",
      "Test Loss: 0.0856, Test Accuracy: 98.20%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1014/1032 (98.26%)\n",
      "Class: 3               - Correct: 985/1010 (97.52%)\n",
      "Class: 4               - Correct: 959/982 (97.66%)\n",
      "Class: 5               - Correct: 880/892 (98.65%)\n",
      "Class: 6               - Correct: 942/958 (98.33%)\n",
      "Class: 7               - Correct: 1007/1028 (97.96%)\n",
      "Class: 8               - Correct: 942/974 (96.71%)\n",
      "Class: 9               - Correct: 993/1009 (98.41%)\n",
      "\n",
      "\n",
      "Epoch [30/100], Step [100/938], Loss: 0.0758, Accuracy: 97.98%\n",
      "Epoch [30/100], Step [200/938], Loss: 0.0881, Accuracy: 97.72%\n",
      "Epoch [30/100], Step [300/938], Loss: 0.0938, Accuracy: 97.64%\n",
      "Epoch [30/100], Step [400/938], Loss: 0.0923, Accuracy: 97.66%\n",
      "Epoch [30/100], Step [500/938], Loss: 0.0867, Accuracy: 97.77%\n",
      "Epoch [30/100], Step [600/938], Loss: 0.0847, Accuracy: 97.77%\n",
      "Epoch [30/100], Step [700/938], Loss: 0.0848, Accuracy: 97.78%\n",
      "Epoch [30/100], Step [800/938], Loss: 0.0840, Accuracy: 97.78%\n",
      "Epoch [30/100], Step [900/938], Loss: 0.0849, Accuracy: 97.74%\n",
      "Epoch [30/100] completed in 9.65 seconds. Average Loss: 0.0850, Accuracy: 97.73%\n",
      "Test Loss: 0.0909, Test Accuracy: 97.96%\n",
      "\n",
      "Class: 0               - Correct: 971/980 (99.08%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1008/1032 (97.67%)\n",
      "Class: 3               - Correct: 992/1010 (98.22%)\n",
      "Class: 4               - Correct: 961/982 (97.86%)\n",
      "Class: 5               - Correct: 870/892 (97.53%)\n",
      "Class: 6               - Correct: 940/958 (98.12%)\n",
      "Class: 7               - Correct: 1004/1028 (97.67%)\n",
      "Class: 8               - Correct: 941/974 (96.61%)\n",
      "Class: 9               - Correct: 983/1009 (97.42%)\n",
      "\n",
      "\n",
      "Epoch [31/100], Step [100/938], Loss: 0.0807, Accuracy: 97.73%\n",
      "Epoch [31/100], Step [200/938], Loss: 0.0756, Accuracy: 97.91%\n",
      "Epoch [31/100], Step [300/938], Loss: 0.0709, Accuracy: 98.02%\n",
      "Epoch [31/100], Step [400/938], Loss: 0.0715, Accuracy: 98.01%\n",
      "Epoch [31/100], Step [500/938], Loss: 0.0748, Accuracy: 97.94%\n",
      "Epoch [31/100], Step [600/938], Loss: 0.0759, Accuracy: 97.93%\n",
      "Epoch [31/100], Step [700/938], Loss: 0.0778, Accuracy: 97.88%\n",
      "Epoch [31/100], Step [800/938], Loss: 0.0796, Accuracy: 97.85%\n",
      "Epoch [31/100], Step [900/938], Loss: 0.0801, Accuracy: 97.84%\n",
      "Epoch [31/100] completed in 9.51 seconds. Average Loss: 0.0803, Accuracy: 97.82%\n",
      "Test Loss: 0.0842, Test Accuracy: 98.11%\n",
      "\n",
      "Class: 0               - Correct: 970/980 (98.98%)\n",
      "Class: 1               - Correct: 1123/1135 (98.94%)\n",
      "Class: 2               - Correct: 1017/1032 (98.55%)\n",
      "Class: 3               - Correct: 988/1010 (97.82%)\n",
      "Class: 4               - Correct: 963/982 (98.07%)\n",
      "Class: 5               - Correct: 880/892 (98.65%)\n",
      "Class: 6               - Correct: 943/958 (98.43%)\n",
      "Class: 7               - Correct: 1001/1028 (97.37%)\n",
      "Class: 8               - Correct: 948/974 (97.33%)\n",
      "Class: 9               - Correct: 978/1009 (96.93%)\n",
      "\n",
      "\n",
      "Epoch [32/100], Step [100/938], Loss: 0.0758, Accuracy: 98.06%\n",
      "Epoch [32/100], Step [200/938], Loss: 0.0821, Accuracy: 97.85%\n",
      "Epoch [32/100], Step [300/938], Loss: 0.0789, Accuracy: 97.95%\n",
      "Epoch [32/100], Step [400/938], Loss: 0.0796, Accuracy: 97.92%\n",
      "Epoch [32/100], Step [500/938], Loss: 0.0802, Accuracy: 97.92%\n",
      "Epoch [32/100], Step [600/938], Loss: 0.0776, Accuracy: 97.97%\n",
      "Epoch [32/100], Step [700/938], Loss: 0.0789, Accuracy: 97.95%\n",
      "Epoch [32/100], Step [800/938], Loss: 0.0787, Accuracy: 97.93%\n",
      "Epoch [32/100], Step [900/938], Loss: 0.0801, Accuracy: 97.88%\n",
      "Epoch [32/100] completed in 9.60 seconds. Average Loss: 0.0809, Accuracy: 97.86%\n",
      "Test Loss: 0.0915, Test Accuracy: 98.06%\n",
      "\n",
      "Class: 0               - Correct: 974/980 (99.39%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1006/1032 (97.48%)\n",
      "Class: 3               - Correct: 989/1010 (97.92%)\n",
      "Class: 4               - Correct: 965/982 (98.27%)\n",
      "Class: 5               - Correct: 874/892 (97.98%)\n",
      "Class: 6               - Correct: 941/958 (98.23%)\n",
      "Class: 7               - Correct: 1008/1028 (98.05%)\n",
      "Class: 8               - Correct: 945/974 (97.02%)\n",
      "Class: 9               - Correct: 978/1009 (96.93%)\n",
      "\n",
      "\n",
      "Epoch [33/100], Step [100/938], Loss: 0.0666, Accuracy: 98.02%\n",
      "Epoch [33/100], Step [200/938], Loss: 0.0755, Accuracy: 97.95%\n",
      "Epoch [33/100], Step [300/938], Loss: 0.0780, Accuracy: 97.92%\n",
      "Epoch [33/100], Step [400/938], Loss: 0.0740, Accuracy: 97.98%\n",
      "Epoch [33/100], Step [500/938], Loss: 0.0750, Accuracy: 98.00%\n",
      "Epoch [33/100], Step [600/938], Loss: 0.0743, Accuracy: 98.02%\n",
      "Epoch [33/100], Step [700/938], Loss: 0.0753, Accuracy: 97.97%\n",
      "Epoch [33/100], Step [800/938], Loss: 0.0770, Accuracy: 97.96%\n",
      "Epoch [33/100], Step [900/938], Loss: 0.0775, Accuracy: 97.95%\n",
      "Epoch [33/100] completed in 9.74 seconds. Average Loss: 0.0777, Accuracy: 97.93%\n",
      "Test Loss: 0.0777, Test Accuracy: 98.36%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1124/1135 (99.03%)\n",
      "Class: 2               - Correct: 1015/1032 (98.35%)\n",
      "Class: 3               - Correct: 993/1010 (98.32%)\n",
      "Class: 4               - Correct: 965/982 (98.27%)\n",
      "Class: 5               - Correct: 870/892 (97.53%)\n",
      "Class: 6               - Correct: 945/958 (98.64%)\n",
      "Class: 7               - Correct: 1013/1028 (98.54%)\n",
      "Class: 8               - Correct: 954/974 (97.95%)\n",
      "Class: 9               - Correct: 984/1009 (97.52%)\n",
      "\n",
      "\n",
      "Epoch [34/100], Step [100/938], Loss: 0.0838, Accuracy: 97.64%\n",
      "Epoch [34/100], Step [200/938], Loss: 0.0811, Accuracy: 97.77%\n",
      "Epoch [34/100], Step [300/938], Loss: 0.0778, Accuracy: 97.91%\n",
      "Epoch [34/100], Step [400/938], Loss: 0.0787, Accuracy: 97.91%\n",
      "Epoch [34/100], Step [500/938], Loss: 0.0801, Accuracy: 97.87%\n",
      "Epoch [34/100], Step [600/938], Loss: 0.0792, Accuracy: 97.88%\n",
      "Epoch [34/100], Step [700/938], Loss: 0.0791, Accuracy: 97.89%\n",
      "Epoch [34/100], Step [800/938], Loss: 0.0799, Accuracy: 97.86%\n",
      "Epoch [34/100], Step [900/938], Loss: 0.0794, Accuracy: 97.86%\n",
      "Epoch [34/100] completed in 9.74 seconds. Average Loss: 0.0788, Accuracy: 97.86%\n",
      "Test Loss: 0.0834, Test Accuracy: 98.29%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1129/1135 (99.47%)\n",
      "Class: 2               - Correct: 1016/1032 (98.45%)\n",
      "Class: 3               - Correct: 991/1010 (98.12%)\n",
      "Class: 4               - Correct: 962/982 (97.96%)\n",
      "Class: 5               - Correct: 873/892 (97.87%)\n",
      "Class: 6               - Correct: 939/958 (98.02%)\n",
      "Class: 7               - Correct: 1006/1028 (97.86%)\n",
      "Class: 8               - Correct: 953/974 (97.84%)\n",
      "Class: 9               - Correct: 987/1009 (97.82%)\n",
      "\n",
      "\n",
      "Epoch [35/100], Step [100/938], Loss: 0.0782, Accuracy: 97.95%\n",
      "Epoch [35/100], Step [200/938], Loss: 0.0786, Accuracy: 97.96%\n",
      "Epoch [35/100], Step [300/938], Loss: 0.0744, Accuracy: 98.02%\n",
      "Epoch [35/100], Step [400/938], Loss: 0.0800, Accuracy: 97.89%\n",
      "Epoch [35/100], Step [500/938], Loss: 0.0788, Accuracy: 97.93%\n",
      "Epoch [35/100], Step [600/938], Loss: 0.0796, Accuracy: 97.91%\n",
      "Epoch [35/100], Step [700/938], Loss: 0.0778, Accuracy: 97.97%\n",
      "Epoch [35/100], Step [800/938], Loss: 0.0776, Accuracy: 97.97%\n",
      "Epoch [35/100], Step [900/938], Loss: 0.0778, Accuracy: 97.96%\n",
      "Epoch [35/100] completed in 9.79 seconds. Average Loss: 0.0783, Accuracy: 97.96%\n",
      "Test Loss: 0.0873, Test Accuracy: 98.23%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1123/1135 (98.94%)\n",
      "Class: 2               - Correct: 1017/1032 (98.55%)\n",
      "Class: 3               - Correct: 992/1010 (98.22%)\n",
      "Class: 4               - Correct: 963/982 (98.07%)\n",
      "Class: 5               - Correct: 877/892 (98.32%)\n",
      "Class: 6               - Correct: 945/958 (98.64%)\n",
      "Class: 7               - Correct: 1011/1028 (98.35%)\n",
      "Class: 8               - Correct: 953/974 (97.84%)\n",
      "Class: 9               - Correct: 970/1009 (96.13%)\n",
      "\n",
      "\n",
      "Epoch [36/100], Step [100/938], Loss: 0.0772, Accuracy: 97.92%\n",
      "Epoch [36/100], Step [200/938], Loss: 0.0756, Accuracy: 98.02%\n",
      "Epoch [36/100], Step [300/938], Loss: 0.0722, Accuracy: 98.06%\n",
      "Epoch [36/100], Step [400/938], Loss: 0.0740, Accuracy: 97.98%\n",
      "Epoch [36/100], Step [500/938], Loss: 0.0755, Accuracy: 97.97%\n",
      "Epoch [36/100], Step [600/938], Loss: 0.0746, Accuracy: 98.01%\n",
      "Epoch [36/100], Step [700/938], Loss: 0.0761, Accuracy: 97.93%\n",
      "Epoch [36/100], Step [800/938], Loss: 0.0762, Accuracy: 97.93%\n",
      "Epoch [36/100], Step [900/938], Loss: 0.0771, Accuracy: 97.94%\n",
      "Epoch [36/100] completed in 9.77 seconds. Average Loss: 0.0768, Accuracy: 97.95%\n",
      "Test Loss: 0.0814, Test Accuracy: 98.05%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1011/1032 (97.97%)\n",
      "Class: 3               - Correct: 994/1010 (98.42%)\n",
      "Class: 4               - Correct: 958/982 (97.56%)\n",
      "Class: 5               - Correct: 872/892 (97.76%)\n",
      "Class: 6               - Correct: 941/958 (98.23%)\n",
      "Class: 7               - Correct: 997/1028 (96.98%)\n",
      "Class: 8               - Correct: 948/974 (97.33%)\n",
      "Class: 9               - Correct: 985/1009 (97.62%)\n",
      "\n",
      "\n",
      "Epoch [37/100], Step [100/938], Loss: 0.0577, Accuracy: 98.39%\n",
      "Epoch [37/100], Step [200/938], Loss: 0.0677, Accuracy: 98.23%\n",
      "Epoch [37/100], Step [300/938], Loss: 0.0668, Accuracy: 98.28%\n",
      "Epoch [37/100], Step [400/938], Loss: 0.0706, Accuracy: 98.17%\n",
      "Epoch [37/100], Step [500/938], Loss: 0.0704, Accuracy: 98.19%\n",
      "Epoch [37/100], Step [600/938], Loss: 0.0711, Accuracy: 98.19%\n",
      "Epoch [37/100], Step [700/938], Loss: 0.0701, Accuracy: 98.20%\n",
      "Epoch [37/100], Step [800/938], Loss: 0.0701, Accuracy: 98.19%\n",
      "Epoch [37/100], Step [900/938], Loss: 0.0721, Accuracy: 98.12%\n",
      "Epoch [37/100] completed in 9.81 seconds. Average Loss: 0.0730, Accuracy: 98.10%\n",
      "Test Loss: 0.0881, Test Accuracy: 98.27%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1128/1135 (99.38%)\n",
      "Class: 2               - Correct: 1018/1032 (98.64%)\n",
      "Class: 3               - Correct: 993/1010 (98.32%)\n",
      "Class: 4               - Correct: 961/982 (97.86%)\n",
      "Class: 5               - Correct: 878/892 (98.43%)\n",
      "Class: 6               - Correct: 937/958 (97.81%)\n",
      "Class: 7               - Correct: 1006/1028 (97.86%)\n",
      "Class: 8               - Correct: 948/974 (97.33%)\n",
      "Class: 9               - Correct: 986/1009 (97.72%)\n",
      "\n",
      "\n",
      "Epoch [38/100], Step [100/938], Loss: 0.0809, Accuracy: 97.94%\n",
      "Epoch [38/100], Step [200/938], Loss: 0.0796, Accuracy: 97.98%\n",
      "Epoch [38/100], Step [300/938], Loss: 0.0793, Accuracy: 97.99%\n",
      "Epoch [38/100], Step [400/938], Loss: 0.0758, Accuracy: 98.03%\n",
      "Epoch [38/100], Step [500/938], Loss: 0.0761, Accuracy: 98.05%\n",
      "Epoch [38/100], Step [600/938], Loss: 0.0787, Accuracy: 98.00%\n",
      "Epoch [38/100], Step [700/938], Loss: 0.0763, Accuracy: 98.03%\n",
      "Epoch [38/100], Step [800/938], Loss: 0.0766, Accuracy: 98.02%\n",
      "Epoch [38/100], Step [900/938], Loss: 0.0780, Accuracy: 97.99%\n",
      "Epoch [38/100] completed in 9.94 seconds. Average Loss: 0.0775, Accuracy: 98.01%\n",
      "Test Loss: 0.0861, Test Accuracy: 98.07%\n",
      "\n",
      "Class: 0               - Correct: 969/980 (98.88%)\n",
      "Class: 1               - Correct: 1127/1135 (99.30%)\n",
      "Class: 2               - Correct: 1012/1032 (98.06%)\n",
      "Class: 3               - Correct: 995/1010 (98.51%)\n",
      "Class: 4               - Correct: 963/982 (98.07%)\n",
      "Class: 5               - Correct: 871/892 (97.65%)\n",
      "Class: 6               - Correct: 943/958 (98.43%)\n",
      "Class: 7               - Correct: 1002/1028 (97.47%)\n",
      "Class: 8               - Correct: 953/974 (97.84%)\n",
      "Class: 9               - Correct: 972/1009 (96.33%)\n",
      "\n",
      "\n",
      "Epoch [39/100], Step [100/938], Loss: 0.0722, Accuracy: 98.02%\n",
      "Epoch [39/100], Step [200/938], Loss: 0.0712, Accuracy: 98.06%\n",
      "Epoch [39/100], Step [300/938], Loss: 0.0732, Accuracy: 98.01%\n",
      "Epoch [39/100], Step [400/938], Loss: 0.0723, Accuracy: 98.08%\n",
      "Epoch [39/100], Step [500/938], Loss: 0.0708, Accuracy: 98.10%\n",
      "Epoch [39/100], Step [600/938], Loss: 0.0706, Accuracy: 98.09%\n",
      "Epoch [39/100], Step [700/938], Loss: 0.0712, Accuracy: 98.08%\n",
      "Epoch [39/100], Step [800/938], Loss: 0.0743, Accuracy: 98.03%\n",
      "Epoch [39/100], Step [900/938], Loss: 0.0750, Accuracy: 98.03%\n",
      "Epoch [39/100] completed in 9.90 seconds. Average Loss: 0.0751, Accuracy: 98.04%\n",
      "Test Loss: 0.0866, Test Accuracy: 98.18%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1122/1135 (98.85%)\n",
      "Class: 2               - Correct: 1011/1032 (97.97%)\n",
      "Class: 3               - Correct: 997/1010 (98.71%)\n",
      "Class: 4               - Correct: 963/982 (98.07%)\n",
      "Class: 5               - Correct: 866/892 (97.09%)\n",
      "Class: 6               - Correct: 938/958 (97.91%)\n",
      "Class: 7               - Correct: 1009/1028 (98.15%)\n",
      "Class: 8               - Correct: 954/974 (97.95%)\n",
      "Class: 9               - Correct: 986/1009 (97.72%)\n",
      "\n",
      "\n",
      "Epoch [40/100], Step [100/938], Loss: 0.0807, Accuracy: 97.59%\n",
      "Epoch [40/100], Step [200/938], Loss: 0.0841, Accuracy: 97.77%\n",
      "Epoch [40/100], Step [300/938], Loss: 0.0749, Accuracy: 97.95%\n",
      "Epoch [40/100], Step [400/938], Loss: 0.0721, Accuracy: 98.05%\n",
      "Epoch [40/100], Step [500/938], Loss: 0.0719, Accuracy: 98.10%\n",
      "Epoch [40/100], Step [600/938], Loss: 0.0735, Accuracy: 98.07%\n",
      "Epoch [40/100], Step [700/938], Loss: 0.0721, Accuracy: 98.07%\n",
      "Epoch [40/100], Step [800/938], Loss: 0.0716, Accuracy: 98.08%\n",
      "Epoch [40/100], Step [900/938], Loss: 0.0727, Accuracy: 98.06%\n",
      "Epoch [40/100] completed in 9.50 seconds. Average Loss: 0.0734, Accuracy: 98.07%\n",
      "Test Loss: 0.0800, Test Accuracy: 98.23%\n",
      "\n",
      "Class: 0               - Correct: 974/980 (99.39%)\n",
      "Class: 1               - Correct: 1121/1135 (98.77%)\n",
      "Class: 2               - Correct: 1011/1032 (97.97%)\n",
      "Class: 3               - Correct: 995/1010 (98.51%)\n",
      "Class: 4               - Correct: 956/982 (97.35%)\n",
      "Class: 5               - Correct: 875/892 (98.09%)\n",
      "Class: 6               - Correct: 946/958 (98.75%)\n",
      "Class: 7               - Correct: 1005/1028 (97.76%)\n",
      "Class: 8               - Correct: 948/974 (97.33%)\n",
      "Class: 9               - Correct: 992/1009 (98.32%)\n",
      "\n",
      "\n",
      "Epoch [41/100], Step [100/938], Loss: 0.0797, Accuracy: 98.08%\n",
      "Epoch [41/100], Step [200/938], Loss: 0.0821, Accuracy: 97.84%\n",
      "Epoch [41/100], Step [300/938], Loss: 0.0775, Accuracy: 97.98%\n",
      "Epoch [41/100], Step [400/938], Loss: 0.0767, Accuracy: 98.00%\n",
      "Epoch [41/100], Step [500/938], Loss: 0.0743, Accuracy: 98.07%\n",
      "Epoch [41/100], Step [600/938], Loss: 0.0735, Accuracy: 98.08%\n",
      "Epoch [41/100], Step [700/938], Loss: 0.0743, Accuracy: 98.06%\n",
      "Epoch [41/100], Step [800/938], Loss: 0.0750, Accuracy: 98.04%\n",
      "Epoch [41/100], Step [900/938], Loss: 0.0757, Accuracy: 98.03%\n",
      "Epoch [41/100] completed in 9.24 seconds. Average Loss: 0.0759, Accuracy: 98.03%\n",
      "Test Loss: 0.0858, Test Accuracy: 98.11%\n",
      "\n",
      "Class: 0               - Correct: 970/980 (98.98%)\n",
      "Class: 1               - Correct: 1123/1135 (98.94%)\n",
      "Class: 2               - Correct: 1011/1032 (97.97%)\n",
      "Class: 3               - Correct: 993/1010 (98.32%)\n",
      "Class: 4               - Correct: 955/982 (97.25%)\n",
      "Class: 5               - Correct: 874/892 (97.98%)\n",
      "Class: 6               - Correct: 942/958 (98.33%)\n",
      "Class: 7               - Correct: 1000/1028 (97.28%)\n",
      "Class: 8               - Correct: 955/974 (98.05%)\n",
      "Class: 9               - Correct: 988/1009 (97.92%)\n",
      "\n",
      "\n",
      "Epoch [42/100], Step [100/938], Loss: 0.0779, Accuracy: 98.33%\n",
      "Epoch [42/100], Step [200/938], Loss: 0.0684, Accuracy: 98.32%\n",
      "Epoch [42/100], Step [300/938], Loss: 0.0695, Accuracy: 98.27%\n",
      "Epoch [42/100], Step [400/938], Loss: 0.0695, Accuracy: 98.23%\n",
      "Epoch [42/100], Step [500/938], Loss: 0.0724, Accuracy: 98.20%\n",
      "Epoch [42/100], Step [600/938], Loss: 0.0751, Accuracy: 98.15%\n",
      "Epoch [42/100], Step [700/938], Loss: 0.0752, Accuracy: 98.12%\n",
      "Epoch [42/100], Step [800/938], Loss: 0.0766, Accuracy: 98.06%\n",
      "Epoch [42/100], Step [900/938], Loss: 0.0759, Accuracy: 98.06%\n",
      "Epoch [42/100] completed in 9.08 seconds. Average Loss: 0.0756, Accuracy: 98.08%\n",
      "Test Loss: 0.0808, Test Accuracy: 98.38%\n",
      "\n",
      "Class: 0               - Correct: 975/980 (99.49%)\n",
      "Class: 1               - Correct: 1123/1135 (98.94%)\n",
      "Class: 2               - Correct: 1022/1032 (99.03%)\n",
      "Class: 3               - Correct: 998/1010 (98.81%)\n",
      "Class: 4               - Correct: 967/982 (98.47%)\n",
      "Class: 5               - Correct: 872/892 (97.76%)\n",
      "Class: 6               - Correct: 945/958 (98.64%)\n",
      "Class: 7               - Correct: 1009/1028 (98.15%)\n",
      "Class: 8               - Correct: 950/974 (97.54%)\n",
      "Class: 9               - Correct: 977/1009 (96.83%)\n",
      "\n",
      "\n",
      "Epoch [43/100], Step [100/938], Loss: 0.0533, Accuracy: 98.34%\n",
      "Epoch [43/100], Step [200/938], Loss: 0.0572, Accuracy: 98.19%\n",
      "Epoch [43/100], Step [300/938], Loss: 0.0599, Accuracy: 98.22%\n",
      "Epoch [43/100], Step [400/938], Loss: 0.0627, Accuracy: 98.17%\n",
      "Epoch [43/100], Step [500/938], Loss: 0.0656, Accuracy: 98.13%\n",
      "Epoch [43/100], Step [600/938], Loss: 0.0681, Accuracy: 98.11%\n",
      "Epoch [43/100], Step [700/938], Loss: 0.0686, Accuracy: 98.14%\n",
      "Epoch [43/100], Step [800/938], Loss: 0.0676, Accuracy: 98.17%\n",
      "Epoch [43/100], Step [900/938], Loss: 0.0687, Accuracy: 98.16%\n",
      "Epoch [43/100] completed in 9.18 seconds. Average Loss: 0.0687, Accuracy: 98.16%\n",
      "Test Loss: 0.0828, Test Accuracy: 98.32%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1129/1135 (99.47%)\n",
      "Class: 2               - Correct: 1018/1032 (98.64%)\n",
      "Class: 3               - Correct: 1000/1010 (99.01%)\n",
      "Class: 4               - Correct: 963/982 (98.07%)\n",
      "Class: 5               - Correct: 872/892 (97.76%)\n",
      "Class: 6               - Correct: 939/958 (98.02%)\n",
      "Class: 7               - Correct: 1005/1028 (97.76%)\n",
      "Class: 8               - Correct: 951/974 (97.64%)\n",
      "Class: 9               - Correct: 983/1009 (97.42%)\n",
      "\n",
      "\n",
      "Epoch [44/100], Step [100/938], Loss: 0.0674, Accuracy: 98.20%\n",
      "Epoch [44/100], Step [200/938], Loss: 0.0678, Accuracy: 98.19%\n",
      "Epoch [44/100], Step [300/938], Loss: 0.0716, Accuracy: 98.11%\n",
      "Epoch [44/100], Step [400/938], Loss: 0.0725, Accuracy: 98.09%\n",
      "Epoch [44/100], Step [500/938], Loss: 0.0713, Accuracy: 98.12%\n",
      "Epoch [44/100], Step [600/938], Loss: 0.0687, Accuracy: 98.21%\n",
      "Epoch [44/100], Step [700/938], Loss: 0.0688, Accuracy: 98.21%\n",
      "Epoch [44/100], Step [800/938], Loss: 0.0698, Accuracy: 98.18%\n",
      "Epoch [44/100], Step [900/938], Loss: 0.0696, Accuracy: 98.16%\n",
      "Epoch [44/100] completed in 9.63 seconds. Average Loss: 0.0691, Accuracy: 98.18%\n",
      "Test Loss: 0.0951, Test Accuracy: 98.23%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1121/1135 (98.77%)\n",
      "Class: 2               - Correct: 1016/1032 (98.45%)\n",
      "Class: 3               - Correct: 994/1010 (98.42%)\n",
      "Class: 4               - Correct: 963/982 (98.07%)\n",
      "Class: 5               - Correct: 877/892 (98.32%)\n",
      "Class: 6               - Correct: 942/958 (98.33%)\n",
      "Class: 7               - Correct: 1010/1028 (98.25%)\n",
      "Class: 8               - Correct: 941/974 (96.61%)\n",
      "Class: 9               - Correct: 987/1009 (97.82%)\n",
      "\n",
      "\n",
      "Epoch [45/100], Step [100/938], Loss: 0.0714, Accuracy: 98.12%\n",
      "Epoch [45/100], Step [200/938], Loss: 0.0689, Accuracy: 98.14%\n",
      "Epoch [45/100], Step [300/938], Loss: 0.0722, Accuracy: 98.12%\n",
      "Epoch [45/100], Step [400/938], Loss: 0.0748, Accuracy: 98.11%\n",
      "Epoch [45/100], Step [500/938], Loss: 0.0735, Accuracy: 98.09%\n",
      "Epoch [45/100], Step [600/938], Loss: 0.0748, Accuracy: 98.06%\n",
      "Epoch [45/100], Step [700/938], Loss: 0.0750, Accuracy: 98.07%\n",
      "Epoch [45/100], Step [800/938], Loss: 0.0753, Accuracy: 98.07%\n",
      "Epoch [45/100], Step [900/938], Loss: 0.0757, Accuracy: 98.06%\n",
      "Epoch [45/100] completed in 9.55 seconds. Average Loss: 0.0750, Accuracy: 98.06%\n",
      "Test Loss: 0.1005, Test Accuracy: 98.00%\n",
      "\n",
      "Class: 0               - Correct: 973/980 (99.29%)\n",
      "Class: 1               - Correct: 1126/1135 (99.21%)\n",
      "Class: 2               - Correct: 1017/1032 (98.55%)\n",
      "Class: 3               - Correct: 991/1010 (98.12%)\n",
      "Class: 4               - Correct: 968/982 (98.57%)\n",
      "Class: 5               - Correct: 872/892 (97.76%)\n",
      "Class: 6               - Correct: 936/958 (97.70%)\n",
      "Class: 7               - Correct: 997/1028 (96.98%)\n",
      "Class: 8               - Correct: 940/974 (96.51%)\n",
      "Class: 9               - Correct: 980/1009 (97.13%)\n",
      "\n",
      "\n",
      "Epoch [46/100], Step [100/938], Loss: 0.0697, Accuracy: 98.19%\n",
      "Epoch [46/100], Step [200/938], Loss: 0.0715, Accuracy: 98.16%\n",
      "Epoch [46/100], Step [300/938], Loss: 0.0693, Accuracy: 98.19%\n",
      "Epoch [46/100], Step [400/938], Loss: 0.0683, Accuracy: 98.17%\n",
      "Epoch [46/100], Step [500/938], Loss: 0.0690, Accuracy: 98.15%\n",
      "Epoch [46/100], Step [600/938], Loss: 0.0676, Accuracy: 98.17%\n",
      "Epoch [46/100], Step [700/938], Loss: 0.0688, Accuracy: 98.15%\n",
      "Epoch [46/100], Step [800/938], Loss: 0.0689, Accuracy: 98.17%\n",
      "Epoch [46/100], Step [900/938], Loss: 0.0685, Accuracy: 98.17%\n",
      "Epoch [46/100] completed in 9.69 seconds. Average Loss: 0.0687, Accuracy: 98.15%\n",
      "Test Loss: 0.0948, Test Accuracy: 98.34%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1121/1135 (98.77%)\n",
      "Class: 2               - Correct: 1013/1032 (98.16%)\n",
      "Class: 3               - Correct: 998/1010 (98.81%)\n",
      "Class: 4               - Correct: 963/982 (98.07%)\n",
      "Class: 5               - Correct: 877/892 (98.32%)\n",
      "Class: 6               - Correct: 947/958 (98.85%)\n",
      "Class: 7               - Correct: 1007/1028 (97.96%)\n",
      "Class: 8               - Correct: 953/974 (97.84%)\n",
      "Class: 9               - Correct: 983/1009 (97.42%)\n",
      "\n",
      "\n",
      "Epoch [47/100], Step [100/938], Loss: 0.0863, Accuracy: 97.94%\n",
      "Epoch [47/100], Step [200/938], Loss: 0.0751, Accuracy: 98.15%\n",
      "Epoch [47/100], Step [300/938], Loss: 0.0729, Accuracy: 98.22%\n",
      "Epoch [47/100], Step [400/938], Loss: 0.0712, Accuracy: 98.25%\n",
      "Epoch [47/100], Step [500/938], Loss: 0.0689, Accuracy: 98.27%\n",
      "Epoch [47/100], Step [600/938], Loss: 0.0700, Accuracy: 98.25%\n",
      "Epoch [47/100], Step [700/938], Loss: 0.0703, Accuracy: 98.24%\n",
      "Epoch [47/100], Step [800/938], Loss: 0.0688, Accuracy: 98.26%\n",
      "Epoch [47/100], Step [900/938], Loss: 0.0700, Accuracy: 98.26%\n",
      "Epoch [47/100] completed in 9.78 seconds. Average Loss: 0.0709, Accuracy: 98.23%\n",
      "Test Loss: 0.0824, Test Accuracy: 98.41%\n",
      "\n",
      "Class: 0               - Correct: 971/980 (99.08%)\n",
      "Class: 1               - Correct: 1120/1135 (98.68%)\n",
      "Class: 2               - Correct: 1016/1032 (98.45%)\n",
      "Class: 3               - Correct: 998/1010 (98.81%)\n",
      "Class: 4               - Correct: 967/982 (98.47%)\n",
      "Class: 5               - Correct: 871/892 (97.65%)\n",
      "Class: 6               - Correct: 942/958 (98.33%)\n",
      "Class: 7               - Correct: 1016/1028 (98.83%)\n",
      "Class: 8               - Correct: 955/974 (98.05%)\n",
      "Class: 9               - Correct: 985/1009 (97.62%)\n",
      "\n",
      "\n",
      "Epoch [48/100], Step [100/938], Loss: 0.0696, Accuracy: 98.05%\n",
      "Epoch [48/100], Step [200/938], Loss: 0.0680, Accuracy: 98.18%\n",
      "Epoch [48/100], Step [300/938], Loss: 0.0663, Accuracy: 98.18%\n",
      "Epoch [48/100], Step [400/938], Loss: 0.0662, Accuracy: 98.22%\n",
      "Epoch [48/100], Step [500/938], Loss: 0.0669, Accuracy: 98.16%\n",
      "Epoch [48/100], Step [600/938], Loss: 0.0648, Accuracy: 98.21%\n",
      "Epoch [48/100], Step [700/938], Loss: 0.0660, Accuracy: 98.20%\n",
      "Epoch [48/100], Step [800/938], Loss: 0.0665, Accuracy: 98.18%\n",
      "Epoch [48/100], Step [900/938], Loss: 0.0659, Accuracy: 98.19%\n",
      "Epoch [48/100] completed in 9.80 seconds. Average Loss: 0.0661, Accuracy: 98.20%\n",
      "Test Loss: 0.0910, Test Accuracy: 98.31%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1127/1135 (99.30%)\n",
      "Class: 2               - Correct: 1016/1032 (98.45%)\n",
      "Class: 3               - Correct: 993/1010 (98.32%)\n",
      "Class: 4               - Correct: 964/982 (98.17%)\n",
      "Class: 5               - Correct: 865/892 (96.97%)\n",
      "Class: 6               - Correct: 947/958 (98.85%)\n",
      "Class: 7               - Correct: 1002/1028 (97.47%)\n",
      "Class: 8               - Correct: 957/974 (98.25%)\n",
      "Class: 9               - Correct: 988/1009 (97.92%)\n",
      "\n",
      "\n",
      "Epoch [49/100], Step [100/938], Loss: 0.0633, Accuracy: 98.36%\n",
      "Epoch [49/100], Step [200/938], Loss: 0.0595, Accuracy: 98.47%\n",
      "Epoch [49/100], Step [300/938], Loss: 0.0641, Accuracy: 98.27%\n",
      "Epoch [49/100], Step [400/938], Loss: 0.0632, Accuracy: 98.31%\n",
      "Epoch [49/100], Step [500/938], Loss: 0.0648, Accuracy: 98.33%\n",
      "Epoch [49/100], Step [600/938], Loss: 0.0680, Accuracy: 98.30%\n",
      "Epoch [49/100], Step [700/938], Loss: 0.0688, Accuracy: 98.25%\n",
      "Epoch [49/100], Step [800/938], Loss: 0.0693, Accuracy: 98.22%\n",
      "Epoch [49/100], Step [900/938], Loss: 0.0703, Accuracy: 98.20%\n",
      "Epoch [49/100] completed in 9.85 seconds. Average Loss: 0.0699, Accuracy: 98.21%\n",
      "Test Loss: 0.0949, Test Accuracy: 98.30%\n",
      "\n",
      "Class: 0               - Correct: 970/980 (98.98%)\n",
      "Class: 1               - Correct: 1125/1135 (99.12%)\n",
      "Class: 2               - Correct: 1017/1032 (98.55%)\n",
      "Class: 3               - Correct: 987/1010 (97.72%)\n",
      "Class: 4               - Correct: 968/982 (98.57%)\n",
      "Class: 5               - Correct: 870/892 (97.53%)\n",
      "Class: 6               - Correct: 947/958 (98.85%)\n",
      "Class: 7               - Correct: 1014/1028 (98.64%)\n",
      "Class: 8               - Correct: 949/974 (97.43%)\n",
      "Class: 9               - Correct: 983/1009 (97.42%)\n",
      "\n",
      "\n",
      "Epoch [50/100], Step [100/938], Loss: 0.0612, Accuracy: 98.25%\n",
      "Epoch [50/100], Step [200/938], Loss: 0.0619, Accuracy: 98.27%\n",
      "Epoch [50/100], Step [300/938], Loss: 0.0644, Accuracy: 98.22%\n",
      "Epoch [50/100], Step [400/938], Loss: 0.0617, Accuracy: 98.27%\n",
      "Epoch [50/100], Step [500/938], Loss: 0.0640, Accuracy: 98.21%\n",
      "Epoch [50/100], Step [600/938], Loss: 0.0649, Accuracy: 98.21%\n",
      "Epoch [50/100], Step [700/938], Loss: 0.0675, Accuracy: 98.18%\n",
      "Epoch [50/100], Step [800/938], Loss: 0.0684, Accuracy: 98.16%\n",
      "Epoch [50/100], Step [900/938], Loss: 0.0696, Accuracy: 98.16%\n",
      "Epoch [50/100] completed in 9.79 seconds. Average Loss: 0.0705, Accuracy: 98.14%\n",
      "Test Loss: 0.0901, Test Accuracy: 98.32%\n",
      "\n",
      "Class: 0               - Correct: 972/980 (99.18%)\n",
      "Class: 1               - Correct: 1125/1135 (99.12%)\n",
      "Class: 2               - Correct: 1013/1032 (98.16%)\n",
      "Class: 3               - Correct: 994/1010 (98.42%)\n",
      "Class: 4               - Correct: 969/982 (98.68%)\n",
      "Class: 5               - Correct: 872/892 (97.76%)\n",
      "Class: 6               - Correct: 944/958 (98.54%)\n",
      "Class: 7               - Correct: 1007/1028 (97.96%)\n",
      "Class: 8               - Correct: 952/974 (97.74%)\n",
      "Class: 9               - Correct: 984/1009 (97.52%)\n",
      "\n",
      "\n",
      "Training completed. Best Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets \n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"This model runs on {device}\")\n",
    "\n",
    "class_names = [str(i) for i in range(10)]  # MNIST has digits 0-9\n",
    "\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(28 * 28, 512)  # Input layer\n",
    "        self.dropout1 = nn.Dropout(p=0.5)      # Dropout layer for regularization\n",
    "        self.layer2 = nn.Linear(512, 256)       # Second layer\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.layer3 = nn.Linear(256, 128)       # Third layer\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.layer4 = nn.Linear(128, 10)        # Output layer\n",
    "        self.relu = nn.ReLU()                   # Activation function\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, 28 * 28)  # Flatten the input\n",
    "        res = self.layer1(X)\n",
    "        res = self.relu(res)\n",
    "        res = self.dropout1(res)  # Apply dropout\n",
    "        res = self.layer2(res)\n",
    "        res = self.relu(res)\n",
    "        res = self.dropout2(res)\n",
    "        res = self.layer3(res)\n",
    "        res = self.relu(res)\n",
    "        res = self.dropout3(res)\n",
    "        res = self.layer4(res)  # Output logits\n",
    "        return res \n",
    "\n",
    "def Prepare_Data(batch_size_train=64, batch_size_test=1024):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Normalize with MNIST mean and std\n",
    "    ])\n",
    "    mnist_trainset = datasets.MNIST(\n",
    "        root='../data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    train_loader = DataLoader(mnist_trainset, batch_size=batch_size_train, shuffle=True)\n",
    "    mnist_testset = datasets.MNIST(\n",
    "        root='../data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    test_loader = DataLoader(mnist_testset, batch_size=batch_size_test, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "model = ComplexModel().to(device=device)\n",
    "print(model)\n",
    " \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Lower learning rate\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()            \n",
    "        outputs = model(data)           \n",
    "        loss = criterion(outputs, target)   \n",
    "        loss.backward()                \n",
    "        optimizer.step()                 \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/100], Step [{batch_idx + 1}/{len(train_loader)}], '\n",
    "                  f'Loss: {running_loss / (batch_idx + 1):.4f}, '\n",
    "                  f'Accuracy: {100 * correct / total:.2f}%')\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f'Epoch [{epoch}/100] completed in {epoch_time:.2f} seconds. '\n",
    "          f'Average Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "def test(model, device, test_loader, criterion, class_names):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for _ in range(len(class_names)))\n",
    "    class_total = list(0. for _ in range(len(class_names)))\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            c = (predicted == target).squeeze()\n",
    "            for i in range(len(target)):\n",
    "                label = target[i]\n",
    "                class_correct[label] += (predicted[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    average_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.2f}%\\n')\n",
    "    for i in range(len(class_names)):\n",
    "        if class_total[i] > 0:\n",
    "            print(f'Class: {class_names[i]:15s} - Correct: {int(class_correct[i])}/{int(class_total[i])} '\n",
    "                  f'({100 * class_correct[i] / class_total[i]:.2f}%)')\n",
    "        else:\n",
    "            print(f'Class: {class_names[i]:15s} - No samples.')\n",
    "    print(\"\\n\")\n",
    "    return accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, test_loader = Prepare_Data()\n",
    "    best_accuracy = 0.0\n",
    "    num_epochs = 50\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        accuracy = test(model, device, test_loader, criterion, class_names)\n",
    "        # if accuracy > best_accuracy:\n",
    "        #     best_accuracy = accuracy\n",
    "        #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "        #     print(f'Best model saved with accuracy: {best_accuracy:.2f}%\\n')\n",
    "    print(f'Training completed. Best Test Accuracy: {best_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569882"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_parameters_original= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_parameters_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametrizedLinear(\n",
       "  in_features=256, out_features=128, bias=True\n",
       "  (parametrizations): ModuleDict(\n",
       "    (weight): ParametrizationList(\n",
       "      (0): LoRA_Scratch()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "class LoRA_Scratch(nn.Module):\n",
    "    def __init__(self, Layer_dims, rank, alpha= 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        feature_in, feature_out = Layer_dims.weight.shape\n",
    "\n",
    "        self.A = nn.Parameter(torch.zeros(feature_in, rank).to(device=device))\n",
    "        self.A = self.A.to(device)\n",
    "\n",
    "        self.B = nn.Parameter(torch.zeros(rank, feature_out).to(device=device))\n",
    "        self.A = self.A.to(device)\n",
    "\n",
    "        self.scale = alpha/rank\n",
    "        self.LoRA = True\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        if self.LoRA:\n",
    "            return  X + (self.A @ self.B) * self.scale\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "def LoRA_convertor(layer, rank=1, lora_alpha=1):\n",
    "    return LoRA_Scratch(\n",
    "        layer, rank=rank, alpha=lora_alpha\n",
    "    )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.layer1, \"weight\", LoRA_convertor(model.layer1)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.layer2, \"weight\", LoRA_convertor(model.layer2)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.layer3, \"weight\", LoRA_convertor(model.layer3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametrizedLinear(\n",
       "  in_features=784, out_features=512, bias=True\n",
       "  (parametrizations): ModuleDict(\n",
       "    (weight): ParametrizationList(\n",
       "      (0): LoRA_Scratch()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "class LoRA_Scratch(nn.Module):\n",
    "    def __init__(self, Layer_dims, rank, alpha= 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        feature_in, feature_out = Layer_dims.weight.shape\n",
    "\n",
    "        self.A = nn.Parameter(torch.zeros(feature_in, rank).to(device=device))\n",
    "        self.A = self.A.to(device)\n",
    "\n",
    "        self.B = nn.Parameter(torch.zeros(rank, feature_out).to(device=device))\n",
    "        self.A = self.A.to(device)\n",
    "\n",
    "        self.scale = alpha/rank\n",
    "        self.LoRA = True\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        if self.LoRA:\n",
    "            return  X + (self.A @ self.B) * self.scale\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "def LoRA_convertor(layer, rank=1, lora_alpha=1):\n",
    "    return LoRA_Scratch(\n",
    "        layer, rank=rank, alpha=lora_alpha\n",
    "    )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.layer1, \"weight\", LoRA_convertor(model.layer1)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.layer2, \"weight\", LoRA_convertor(model.layer2)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.layer3, \"weight\", LoRA_convertor(model.layer3)\n",
    ")\n",
    "\n",
    "\n",
    "def Enable_lora(enable= True):\n",
    "    for Layer in [model.layer1, model.layer2, model.layer3]:\n",
    "        Layer.parametrizations['weight'][0].LoRA = True\n",
    "\n",
    "def Disable_lora(enable= True):\n",
    "    for Layer in [model.layer1, model.layer2, model.layer3]:\n",
    "        Layer.parametrizations['weight'][0].LoRA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([512, 784]) + B: torch.Size([512]) + Lora_A: torch.Size([512, 1]) + Lora_B: torch.Size([1, 784])\n",
      "Layer 2: W: torch.Size([256, 512]) + B: torch.Size([256]) + Lora_A: torch.Size([256, 1]) + Lora_B: torch.Size([1, 512])\n",
      "Layer 3: W: torch.Size([128, 256]) + B: torch.Size([128]) + Lora_A: torch.Size([128, 1]) + Lora_B: torch.Size([1, 256])\n",
      "Total number of parameters (original): 566,144\n",
      "Total number of parameters (original + LoRA): 568,592\n",
      "Parameters introduced by LoRA: 2,448\n",
      "Parameters incremment: 0.432%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "class LoRA_Scratch(nn.Module):\n",
    "    def __init__(self, Layer_dims, rank, alpha= 0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        feature_in, feature_out = Layer_dims.weight.shape\n",
    "\n",
    "        self.A = nn.Parameter(torch.zeros(feature_in, rank).to(device=device))\n",
    "        self.A = self.A.to(device)\n",
    "\n",
    "        self.B = nn.Parameter(torch.zeros(rank, feature_out).to(device=device))\n",
    "        self.A = self.A.to(device)\n",
    "\n",
    "        self.scale = alpha/rank\n",
    "        self.LoRA = True\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        if self.LoRA:\n",
    "            return  X + (self.A @ self.B) * self.scale\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "def LoRA_convertor(layer, rank=1, lora_alpha=1):\n",
    "    return LoRA_Scratch(\n",
    "        layer, rank=rank, alpha=lora_alpha\n",
    "    )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.layer1, \"weight\", LoRA_convertor(model.layer1)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.layer2, \"weight\", LoRA_convertor(model.layer2)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.layer3, \"weight\", LoRA_convertor(model.layer3)\n",
    ")\n",
    "\n",
    "\n",
    "def Enable_lora(enable= True):\n",
    "    for Layer in [model.layer1, model.layer2, model.layer3]:\n",
    "        Layer.parametrizations['weight'][0].LoRA = True\n",
    "\n",
    "def Disable_lora(enable= True):\n",
    "    for Layer in [model.layer1, model.layer2, model.layer3]:\n",
    "        Layer.parametrizations['weight'][0].LoRA = False\n",
    "\n",
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "for index, layer in enumerate([model.layer1, model.layer2, model.layer3]):\n",
    "    total_parameters_lora += layer.parametrizations[\"weight\"][0].A.nelement() + layer.parametrizations[\"weight\"][0].B.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(\n",
    "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].B.shape}'\n",
    "    )\n",
    "# The non-LoRA parameters count must match the original network\n",
    "# assert total_parameters_non_lora == total_parameters_original, 'not matched'\n",
    "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
    "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566144"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_parameters_non_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569882"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_parameters_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model runs on cuda\n",
      "Layer 1: W: torch.Size([100, 784]) + B: torch.Size([100]) + Lora_A: torch.Size([100, 1]) + Lora_B: torch.Size([1, 784])\n",
      "Layer 2: W: torch.Size([100, 100]) + B: torch.Size([100]) + Lora_A: torch.Size([100, 1]) + Lora_B: torch.Size([1, 100])\n",
      "Layer 3: W: torch.Size([10, 100]) + B: torch.Size([10]) + Lora_A: torch.Size([10, 1]) + Lora_B: torch.Size([1, 100])\n",
      "Total number of parameters (original): 89,610\n",
      "Total number of parameters (original + LoRA): 90,804\n",
      "Parameters introduced by LoRA: 1,194\n",
      "Parameters increment: 1.332%\n",
      "Freezing non-LoRA parameter layer1.bias\n",
      "Freezing non-LoRA parameter layer1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer1.parametrizations.weight.0.A\n",
      "Freezing non-LoRA parameter layer1.parametrizations.weight.0.B\n",
      "Freezing non-LoRA parameter layer2.bias\n",
      "Freezing non-LoRA parameter layer2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer2.parametrizations.weight.0.A\n",
      "Freezing non-LoRA parameter layer2.parametrizations.weight.0.B\n",
      "Freezing non-LoRA parameter layer3.bias\n",
      "Freezing non-LoRA parameter layer3.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer3.parametrizations.weight.0.A\n",
      "Freezing non-LoRA parameter layer3.parametrizations.weight.0.B\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 140\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    139\u001b[0m         Enable_lora(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Enable LoRA during training\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[76], line 121\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[0;32m    119\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(data)           \n\u001b[0;32m    120\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target)   \n\u001b[1;32m--> 121\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                \n\u001b[0;32m    122\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()                 \n\u001b[0;32m    123\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"This model runs on {device}\")\n",
    "\n",
    "class LoRA_Scratch(nn.Module):\n",
    "    def __init__(self, Layer_dims, rank, alpha=0.5):\n",
    "        super().__init__()\n",
    "        feature_in, feature_out = Layer_dims.weight.shape\n",
    "\n",
    "        self.A = nn.Parameter(torch.zeros(feature_in, rank).to(device=device))\n",
    "        self.B = nn.Parameter(torch.zeros(rank, feature_out).to(device=device))\n",
    "        \n",
    "        self.scale = alpha / rank\n",
    "        self.LoRA = True\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.LoRA:\n",
    "            return X + (self.A @ self.B) * self.scale\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "def LoRA_convertor(layer, rank=1, lora_alpha=1):\n",
    "    return LoRA_Scratch(\n",
    "        layer, rank=rank, alpha=lora_alpha\n",
    "    )\n",
    "\n",
    "# Example model structure\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(28*28, 100)\n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, 28*28)\n",
    "        X = torch.relu(self.layer1(X))\n",
    "        X = torch.relu(self.layer2(X))\n",
    "        X = self.layer3(X)\n",
    "        return X\n",
    "\n",
    "model = Model().to(device=device)\n",
    "\n",
    "# Register LoRA parameters\n",
    "parametrize.register_parametrization(\n",
    "    model.layer1, \"weight\", LoRA_convertor(model.layer1)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.layer2, \"weight\", LoRA_convertor(model.layer2)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.layer3, \"weight\", LoRA_convertor(model.layer3)\n",
    ")\n",
    "\n",
    "def Enable_lora(enable=True):\n",
    "    for Layer in [model.layer1, model.layer2, model.layer3]:\n",
    "        Layer.parametrizations['weight'][0].LoRA = enable\n",
    "\n",
    "def Disable_lora(enable=True):\n",
    "    for Layer in [model.layer1, model.layer2, model.layer3]:\n",
    "        Layer.parametrizations['weight'][0].LoRA = not enable\n",
    "\n",
    "# Count parameters\n",
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "for index, layer in enumerate([model.layer1, model.layer2, model.layer3]):\n",
    "    total_parameters_lora += layer.parametrizations[\"weight\"][0].A.nelement() + layer.parametrizations[\"weight\"][0].B.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + (layer.bias.nelement() if layer.bias is not None else 0)\n",
    "    print(\n",
    "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape if layer.bias is not None else \"None\"} + '\n",
    "        f'Lora_A: {layer.parametrizations[\"weight\"][0].A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].B.shape}'\n",
    "    )\n",
    "\n",
    "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_increment = (total_parameters_lora / total_parameters_non_lora) * 100 if total_parameters_non_lora > 0 else 0\n",
    "print(f'Parameters increment: {parameters_increment:.3f}%')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Load the MNIST dataset, keeping only the digit 9\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with MNIST mean and std\n",
    "])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "exclude_indices = mnist_trainset.targets == 9\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "\n",
    "# Create a dataloader for the training\n",
    "train_loader = DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()            \n",
    "        outputs = model(data)           \n",
    "        loss = criterion(outputs, target)   \n",
    "        loss.backward()                \n",
    "        optimizer.step()                 \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        if (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch}], Step [{batch_idx+1}/{len(train_loader)}], '\n",
    "                  f'Loss: {running_loss / (batch_idx+1):.4f}, '\n",
    "                  f'Accuracy: {100 * correct / total:.2f}%')\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f'Epoch [{epoch}] completed in {epoch_time:.2f} seconds. '\n",
    "          f'Average Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_epochs = 2\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        Enable_lora(True)  # Enable LoRA during training\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "\n",
    "print('Training completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [10/595], Loss: 2.1827, Accuracy: 59.00%\n",
      "Epoch [1], Step [20/595], Loss: 2.1810, Accuracy: 58.00%\n",
      "Epoch [1], Step [30/595], Loss: 2.1796, Accuracy: 57.67%\n",
      "Epoch [1], Step [40/595], Loss: 2.1789, Accuracy: 58.25%\n",
      "Epoch [1], Step [50/595], Loss: 2.1784, Accuracy: 58.00%\n",
      "Epoch [1], Step [60/595], Loss: 2.1787, Accuracy: 57.50%\n",
      "Epoch [1], Step [70/595], Loss: 2.1780, Accuracy: 57.86%\n",
      "Epoch [1], Step [80/595], Loss: 2.1787, Accuracy: 57.62%\n",
      "Epoch [1], Step [90/595], Loss: 2.1780, Accuracy: 58.22%\n",
      "Epoch [1], Step [100/595], Loss: 2.1781, Accuracy: 58.40%\n",
      "Epoch [1], Step [110/595], Loss: 2.1783, Accuracy: 57.64%\n",
      "Epoch [1], Step [120/595], Loss: 2.1784, Accuracy: 57.58%\n",
      "Epoch [1], Step [130/595], Loss: 2.1786, Accuracy: 57.23%\n",
      "Epoch [1], Step [140/595], Loss: 2.1791, Accuracy: 56.79%\n",
      "Epoch [1], Step [150/595], Loss: 2.1789, Accuracy: 57.07%\n",
      "Epoch [1], Step [160/595], Loss: 2.1785, Accuracy: 57.12%\n",
      "Epoch [1], Step [170/595], Loss: 2.1782, Accuracy: 57.18%\n",
      "Epoch [1], Step [180/595], Loss: 2.1782, Accuracy: 57.06%\n",
      "Epoch [1], Step [190/595], Loss: 2.1781, Accuracy: 57.16%\n",
      "Epoch [1], Step [200/595], Loss: 2.1781, Accuracy: 57.15%\n",
      "Epoch [1], Step [210/595], Loss: 2.1783, Accuracy: 57.24%\n",
      "Epoch [1], Step [220/595], Loss: 2.1784, Accuracy: 57.36%\n",
      "Epoch [1], Step [230/595], Loss: 2.1785, Accuracy: 57.35%\n",
      "Epoch [1], Step [240/595], Loss: 2.1783, Accuracy: 57.46%\n",
      "Epoch [1], Step [250/595], Loss: 2.1783, Accuracy: 57.20%\n",
      "Epoch [1], Step [260/595], Loss: 2.1783, Accuracy: 57.04%\n",
      "Epoch [1], Step [270/595], Loss: 2.1780, Accuracy: 57.15%\n",
      "Epoch [1], Step [280/595], Loss: 2.1779, Accuracy: 57.18%\n",
      "Epoch [1], Step [290/595], Loss: 2.1779, Accuracy: 57.34%\n",
      "Epoch [1], Step [300/595], Loss: 2.1777, Accuracy: 57.70%\n",
      "Epoch [1], Step [310/595], Loss: 2.1778, Accuracy: 57.45%\n",
      "Epoch [1], Step [320/595], Loss: 2.1778, Accuracy: 57.38%\n",
      "Epoch [1], Step [330/595], Loss: 2.1778, Accuracy: 57.15%\n",
      "Epoch [1], Step [340/595], Loss: 2.1777, Accuracy: 57.29%\n",
      "Epoch [1], Step [350/595], Loss: 2.1779, Accuracy: 57.40%\n",
      "Epoch [1], Step [360/595], Loss: 2.1780, Accuracy: 57.14%\n",
      "Epoch [1], Step [370/595], Loss: 2.1781, Accuracy: 57.08%\n",
      "Epoch [1], Step [380/595], Loss: 2.1779, Accuracy: 57.11%\n",
      "Epoch [1], Step [390/595], Loss: 2.1779, Accuracy: 57.10%\n",
      "Epoch [1], Step [400/595], Loss: 2.1779, Accuracy: 56.95%\n",
      "Epoch [1], Step [410/595], Loss: 2.1779, Accuracy: 56.88%\n",
      "Epoch [1], Step [420/595], Loss: 2.1780, Accuracy: 56.76%\n",
      "Epoch [1], Step [430/595], Loss: 2.1781, Accuracy: 56.60%\n",
      "Epoch [1], Step [440/595], Loss: 2.1781, Accuracy: 56.50%\n",
      "Epoch [1], Step [450/595], Loss: 2.1780, Accuracy: 56.49%\n",
      "Epoch [1], Step [460/595], Loss: 2.1779, Accuracy: 56.70%\n",
      "Epoch [1], Step [470/595], Loss: 2.1780, Accuracy: 56.64%\n",
      "Epoch [1], Step [480/595], Loss: 2.1779, Accuracy: 56.71%\n",
      "Epoch [1], Step [490/595], Loss: 2.1778, Accuracy: 56.76%\n",
      "Epoch [1], Step [500/595], Loss: 2.1778, Accuracy: 56.68%\n",
      "Epoch [1], Step [510/595], Loss: 2.1778, Accuracy: 56.88%\n",
      "Epoch [1], Step [520/595], Loss: 2.1778, Accuracy: 56.92%\n",
      "Epoch [1], Step [530/595], Loss: 2.1778, Accuracy: 56.87%\n",
      "Epoch [1], Step [540/595], Loss: 2.1778, Accuracy: 56.94%\n",
      "Epoch [1], Step [550/595], Loss: 2.1778, Accuracy: 56.96%\n",
      "Epoch [1], Step [560/595], Loss: 2.1778, Accuracy: 57.11%\n",
      "Epoch [1], Step [570/595], Loss: 2.1777, Accuracy: 57.12%\n",
      "Epoch [1], Step [580/595], Loss: 2.1777, Accuracy: 57.02%\n",
      "Epoch [1], Step [590/595], Loss: 2.1777, Accuracy: 57.03%\n",
      "Epoch [1] completed in 2.11 seconds. Average Loss: 2.1777, Accuracy: 57.05%\n",
      "Epoch [2], Step [10/595], Loss: 2.1758, Accuracy: 62.00%\n",
      "Epoch [2], Step [20/595], Loss: 2.1737, Accuracy: 62.50%\n",
      "Epoch [2], Step [30/595], Loss: 2.1748, Accuracy: 60.67%\n",
      "Epoch [2], Step [40/595], Loss: 2.1755, Accuracy: 59.50%\n",
      "Epoch [2], Step [50/595], Loss: 2.1762, Accuracy: 57.60%\n",
      "Epoch [2], Step [60/595], Loss: 2.1765, Accuracy: 57.33%\n",
      "Epoch [2], Step [70/595], Loss: 2.1767, Accuracy: 57.43%\n",
      "Epoch [2], Step [80/595], Loss: 2.1766, Accuracy: 57.62%\n",
      "Epoch [2], Step [90/595], Loss: 2.1765, Accuracy: 57.67%\n",
      "Epoch [2], Step [100/595], Loss: 2.1768, Accuracy: 57.50%\n",
      "Epoch [2], Step [110/595], Loss: 2.1772, Accuracy: 57.00%\n",
      "Epoch [2], Step [120/595], Loss: 2.1776, Accuracy: 57.33%\n",
      "Epoch [2], Step [130/595], Loss: 2.1776, Accuracy: 57.23%\n",
      "Epoch [2], Step [140/595], Loss: 2.1773, Accuracy: 57.29%\n",
      "Epoch [2], Step [150/595], Loss: 2.1772, Accuracy: 57.53%\n",
      "Epoch [2], Step [160/595], Loss: 2.1776, Accuracy: 57.56%\n",
      "Epoch [2], Step [170/595], Loss: 2.1776, Accuracy: 57.41%\n",
      "Epoch [2], Step [180/595], Loss: 2.1776, Accuracy: 57.56%\n",
      "Epoch [2], Step [190/595], Loss: 2.1775, Accuracy: 57.42%\n",
      "Epoch [2], Step [200/595], Loss: 2.1775, Accuracy: 57.50%\n",
      "Epoch [2], Step [210/595], Loss: 2.1780, Accuracy: 57.10%\n",
      "Epoch [2], Step [220/595], Loss: 2.1779, Accuracy: 57.09%\n",
      "Epoch [2], Step [230/595], Loss: 2.1779, Accuracy: 57.39%\n",
      "Epoch [2], Step [240/595], Loss: 2.1777, Accuracy: 57.29%\n",
      "Epoch [2], Step [250/595], Loss: 2.1778, Accuracy: 57.08%\n",
      "Epoch [2], Step [260/595], Loss: 2.1776, Accuracy: 57.19%\n",
      "Epoch [2], Step [270/595], Loss: 2.1777, Accuracy: 57.11%\n",
      "Epoch [2], Step [280/595], Loss: 2.1778, Accuracy: 56.93%\n",
      "Epoch [2], Step [290/595], Loss: 2.1777, Accuracy: 57.17%\n",
      "Epoch [2], Step [300/595], Loss: 2.1778, Accuracy: 57.13%\n",
      "Epoch [2], Step [310/595], Loss: 2.1778, Accuracy: 57.16%\n",
      "Epoch [2], Step [320/595], Loss: 2.1778, Accuracy: 56.97%\n",
      "Epoch [2], Step [330/595], Loss: 2.1777, Accuracy: 57.12%\n",
      "Epoch [2], Step [340/595], Loss: 2.1775, Accuracy: 57.18%\n",
      "Epoch [2], Step [350/595], Loss: 2.1777, Accuracy: 56.86%\n",
      "Epoch [2], Step [360/595], Loss: 2.1777, Accuracy: 56.81%\n",
      "Epoch [2], Step [370/595], Loss: 2.1777, Accuracy: 56.76%\n",
      "Epoch [2], Step [380/595], Loss: 2.1776, Accuracy: 56.79%\n",
      "Epoch [2], Step [390/595], Loss: 2.1777, Accuracy: 56.82%\n",
      "Epoch [2], Step [400/595], Loss: 2.1777, Accuracy: 56.70%\n",
      "Epoch [2], Step [410/595], Loss: 2.1777, Accuracy: 56.73%\n",
      "Epoch [2], Step [420/595], Loss: 2.1776, Accuracy: 56.81%\n",
      "Epoch [2], Step [430/595], Loss: 2.1777, Accuracy: 56.51%\n",
      "Epoch [2], Step [440/595], Loss: 2.1778, Accuracy: 56.52%\n",
      "Epoch [2], Step [450/595], Loss: 2.1778, Accuracy: 56.56%\n",
      "Epoch [2], Step [460/595], Loss: 2.1777, Accuracy: 56.70%\n",
      "Epoch [2], Step [470/595], Loss: 2.1778, Accuracy: 56.70%\n",
      "Epoch [2], Step [480/595], Loss: 2.1779, Accuracy: 56.67%\n",
      "Epoch [2], Step [490/595], Loss: 2.1777, Accuracy: 56.88%\n",
      "Epoch [2], Step [500/595], Loss: 2.1777, Accuracy: 56.92%\n",
      "Epoch [2], Step [510/595], Loss: 2.1776, Accuracy: 56.94%\n",
      "Epoch [2], Step [520/595], Loss: 2.1775, Accuracy: 57.00%\n",
      "Epoch [2], Step [530/595], Loss: 2.1775, Accuracy: 57.06%\n",
      "Epoch [2], Step [540/595], Loss: 2.1775, Accuracy: 57.19%\n",
      "Epoch [2], Step [550/595], Loss: 2.1774, Accuracy: 57.24%\n",
      "Epoch [2], Step [560/595], Loss: 2.1775, Accuracy: 57.30%\n",
      "Epoch [2], Step [570/595], Loss: 2.1775, Accuracy: 57.30%\n",
      "Epoch [2], Step [580/595], Loss: 2.1776, Accuracy: 57.12%\n",
      "Epoch [2], Step [590/595], Loss: 2.1777, Accuracy: 57.02%\n",
      "Epoch [2] completed in 2.28 seconds. Average Loss: 2.1777, Accuracy: 57.05%\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "def Enable_lora(enable=True):\n",
    "    for Layer in [model.layer1, model.layer2, model.layer3]:\n",
    "        Layer.parametrizations['weight'][0].LoRA = enable\n",
    "        for param in [Layer.parametrizations['weight'][0].A, Layer.parametrizations['weight'][0].B]:\n",
    "            param.requires_grad = enable  # Ensure LoRA parameters require grad\n",
    "\n",
    "# Train the network with LoRA only on the digit 9\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()            \n",
    "        \n",
    "        # Check that data requires grad\n",
    "        assert data.requires_grad is False, \"Data should not require gradients\"\n",
    "        \n",
    "        outputs = model(data)           \n",
    "        loss = criterion(outputs, target)   \n",
    "        \n",
    "        # Check that loss requires grad\n",
    "        assert loss.requires_grad is True, \"Loss should require gradients\"\n",
    "        \n",
    "        loss.backward()                \n",
    "        optimizer.step()                 \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch}], Step [{batch_idx+1}/{len(train_loader)}], '\n",
    "                  f'Loss: {running_loss / (batch_idx+1):.4f}, '\n",
    "                  f'Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f'Epoch [{epoch}] completed in {epoch_time:.2f} seconds. '\n",
    "          f'Average Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_epochs = 2\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        Enable_lora(True)  # Enable LoRA during training\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "\n",
    "print('Training completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [10/595], Loss: 2.1782, Accuracy: 60.00%, Correct Predictions: 60/100\n",
      "Epoch [1], Step [20/595], Loss: 2.1799, Accuracy: 55.00%, Correct Predictions: 110/200\n",
      "Epoch [1], Step [30/595], Loss: 2.1808, Accuracy: 55.67%, Correct Predictions: 167/300\n",
      "Epoch [1], Step [40/595], Loss: 2.1801, Accuracy: 58.00%, Correct Predictions: 232/400\n",
      "Epoch [1], Step [50/595], Loss: 2.1792, Accuracy: 56.80%, Correct Predictions: 284/500\n",
      "Epoch [1], Step [60/595], Loss: 2.1786, Accuracy: 56.67%, Correct Predictions: 340/600\n",
      "Epoch [1], Step [70/595], Loss: 2.1790, Accuracy: 56.00%, Correct Predictions: 392/700\n",
      "Epoch [1], Step [80/595], Loss: 2.1790, Accuracy: 56.50%, Correct Predictions: 452/800\n",
      "Epoch [1], Step [90/595], Loss: 2.1785, Accuracy: 57.89%, Correct Predictions: 521/900\n",
      "Epoch [1], Step [100/595], Loss: 2.1787, Accuracy: 57.70%, Correct Predictions: 577/1000\n",
      "Epoch [1], Step [110/595], Loss: 2.1781, Accuracy: 57.64%, Correct Predictions: 634/1100\n",
      "Epoch [1], Step [120/595], Loss: 2.1775, Accuracy: 58.00%, Correct Predictions: 696/1200\n",
      "Epoch [1], Step [130/595], Loss: 2.1778, Accuracy: 58.00%, Correct Predictions: 754/1300\n",
      "Epoch [1], Step [140/595], Loss: 2.1777, Accuracy: 58.29%, Correct Predictions: 816/1400\n",
      "Epoch [1], Step [150/595], Loss: 2.1775, Accuracy: 58.67%, Correct Predictions: 880/1500\n",
      "Epoch [1], Step [160/595], Loss: 2.1774, Accuracy: 58.62%, Correct Predictions: 938/1600\n",
      "Epoch [1], Step [170/595], Loss: 2.1777, Accuracy: 58.35%, Correct Predictions: 992/1700\n",
      "Epoch [1], Step [180/595], Loss: 2.1776, Accuracy: 57.94%, Correct Predictions: 1043/1800\n",
      "Epoch [1], Step [190/595], Loss: 2.1777, Accuracy: 57.42%, Correct Predictions: 1091/1900\n",
      "Epoch [1], Step [200/595], Loss: 2.1777, Accuracy: 57.70%, Correct Predictions: 1154/2000\n",
      "Epoch [1], Step [210/595], Loss: 2.1776, Accuracy: 58.00%, Correct Predictions: 1218/2100\n",
      "Epoch [1], Step [220/595], Loss: 2.1776, Accuracy: 57.77%, Correct Predictions: 1271/2200\n",
      "Epoch [1], Step [230/595], Loss: 2.1775, Accuracy: 57.87%, Correct Predictions: 1331/2300\n",
      "Epoch [1], Step [240/595], Loss: 2.1778, Accuracy: 57.71%, Correct Predictions: 1385/2400\n",
      "Epoch [1], Step [250/595], Loss: 2.1777, Accuracy: 57.64%, Correct Predictions: 1441/2500\n",
      "Epoch [1], Step [260/595], Loss: 2.1775, Accuracy: 57.38%, Correct Predictions: 1492/2600\n",
      "Epoch [1], Step [270/595], Loss: 2.1776, Accuracy: 57.37%, Correct Predictions: 1549/2700\n",
      "Epoch [1], Step [280/595], Loss: 2.1777, Accuracy: 57.39%, Correct Predictions: 1607/2800\n",
      "Epoch [1], Step [290/595], Loss: 2.1776, Accuracy: 57.38%, Correct Predictions: 1664/2900\n",
      "Epoch [1], Step [300/595], Loss: 2.1775, Accuracy: 57.33%, Correct Predictions: 1720/3000\n",
      "Epoch [1], Step [310/595], Loss: 2.1775, Accuracy: 57.19%, Correct Predictions: 1773/3100\n",
      "Epoch [1], Step [320/595], Loss: 2.1776, Accuracy: 57.22%, Correct Predictions: 1831/3200\n",
      "Epoch [1], Step [330/595], Loss: 2.1774, Accuracy: 57.67%, Correct Predictions: 1903/3300\n",
      "Epoch [1], Step [340/595], Loss: 2.1774, Accuracy: 57.47%, Correct Predictions: 1954/3400\n",
      "Epoch [1], Step [350/595], Loss: 2.1776, Accuracy: 57.20%, Correct Predictions: 2002/3500\n",
      "Epoch [1], Step [360/595], Loss: 2.1777, Accuracy: 57.00%, Correct Predictions: 2052/3600\n",
      "Epoch [1], Step [370/595], Loss: 2.1776, Accuracy: 56.95%, Correct Predictions: 2107/3700\n",
      "Epoch [1], Step [380/595], Loss: 2.1776, Accuracy: 57.11%, Correct Predictions: 2170/3800\n",
      "Epoch [1], Step [390/595], Loss: 2.1776, Accuracy: 56.97%, Correct Predictions: 2222/3900\n",
      "Epoch [1], Step [400/595], Loss: 2.1776, Accuracy: 57.15%, Correct Predictions: 2286/4000\n",
      "Epoch [1], Step [410/595], Loss: 2.1776, Accuracy: 57.02%, Correct Predictions: 2338/4100\n",
      "Epoch [1], Step [420/595], Loss: 2.1778, Accuracy: 57.00%, Correct Predictions: 2394/4200\n",
      "Epoch [1], Step [430/595], Loss: 2.1778, Accuracy: 56.91%, Correct Predictions: 2447/4300\n",
      "Epoch [1], Step [440/595], Loss: 2.1778, Accuracy: 56.95%, Correct Predictions: 2506/4400\n",
      "Epoch [1], Step [450/595], Loss: 2.1778, Accuracy: 56.91%, Correct Predictions: 2561/4500\n",
      "Epoch [1], Step [460/595], Loss: 2.1779, Accuracy: 56.87%, Correct Predictions: 2616/4600\n",
      "Epoch [1], Step [470/595], Loss: 2.1779, Accuracy: 56.81%, Correct Predictions: 2670/4700\n",
      "Epoch [1], Step [480/595], Loss: 2.1778, Accuracy: 56.92%, Correct Predictions: 2732/4800\n",
      "Epoch [1], Step [490/595], Loss: 2.1779, Accuracy: 56.96%, Correct Predictions: 2791/4900\n",
      "Epoch [1], Step [500/595], Loss: 2.1779, Accuracy: 56.94%, Correct Predictions: 2847/5000\n",
      "Epoch [1], Step [510/595], Loss: 2.1780, Accuracy: 56.90%, Correct Predictions: 2902/5100\n",
      "Epoch [1], Step [520/595], Loss: 2.1780, Accuracy: 56.85%, Correct Predictions: 2956/5200\n",
      "Epoch [1], Step [530/595], Loss: 2.1780, Accuracy: 56.77%, Correct Predictions: 3009/5300\n",
      "Epoch [1], Step [540/595], Loss: 2.1781, Accuracy: 56.67%, Correct Predictions: 3060/5400\n",
      "Epoch [1], Step [550/595], Loss: 2.1780, Accuracy: 56.75%, Correct Predictions: 3121/5500\n",
      "Epoch [1], Step [560/595], Loss: 2.1780, Accuracy: 56.77%, Correct Predictions: 3179/5600\n",
      "Epoch [1], Step [570/595], Loss: 2.1780, Accuracy: 56.84%, Correct Predictions: 3240/5700\n",
      "Epoch [1], Step [580/595], Loss: 2.1778, Accuracy: 56.88%, Correct Predictions: 3299/5800\n",
      "Epoch [1], Step [590/595], Loss: 2.1777, Accuracy: 57.00%, Correct Predictions: 3363/5900\n",
      "Epoch [1] completed in 3.37 seconds. Average Loss: 2.1777, Accuracy: 57.05%\n",
      "Epoch [2], Step [10/595], Loss: 2.1762, Accuracy: 62.00%, Correct Predictions: 62/100\n",
      "Epoch [2], Step [20/595], Loss: 2.1768, Accuracy: 60.50%, Correct Predictions: 121/200\n",
      "Epoch [2], Step [30/595], Loss: 2.1768, Accuracy: 57.33%, Correct Predictions: 172/300\n",
      "Epoch [2], Step [40/595], Loss: 2.1754, Accuracy: 60.25%, Correct Predictions: 241/400\n",
      "Epoch [2], Step [50/595], Loss: 2.1767, Accuracy: 57.60%, Correct Predictions: 288/500\n",
      "Epoch [2], Step [60/595], Loss: 2.1764, Accuracy: 59.33%, Correct Predictions: 356/600\n",
      "Epoch [2], Step [70/595], Loss: 2.1768, Accuracy: 59.14%, Correct Predictions: 414/700\n",
      "Epoch [2], Step [80/595], Loss: 2.1770, Accuracy: 58.75%, Correct Predictions: 470/800\n",
      "Epoch [2], Step [90/595], Loss: 2.1767, Accuracy: 58.89%, Correct Predictions: 530/900\n",
      "Epoch [2], Step [100/595], Loss: 2.1772, Accuracy: 58.40%, Correct Predictions: 584/1000\n",
      "Epoch [2], Step [110/595], Loss: 2.1769, Accuracy: 58.55%, Correct Predictions: 644/1100\n",
      "Epoch [2], Step [120/595], Loss: 2.1776, Accuracy: 58.42%, Correct Predictions: 701/1200\n",
      "Epoch [2], Step [130/595], Loss: 2.1773, Accuracy: 58.62%, Correct Predictions: 762/1300\n",
      "Epoch [2], Step [140/595], Loss: 2.1774, Accuracy: 58.14%, Correct Predictions: 814/1400\n",
      "Epoch [2], Step [150/595], Loss: 2.1773, Accuracy: 58.13%, Correct Predictions: 872/1500\n",
      "Epoch [2], Step [160/595], Loss: 2.1772, Accuracy: 58.44%, Correct Predictions: 935/1600\n",
      "Epoch [2], Step [170/595], Loss: 2.1773, Accuracy: 58.53%, Correct Predictions: 995/1700\n",
      "Epoch [2], Step [180/595], Loss: 2.1772, Accuracy: 58.44%, Correct Predictions: 1052/1800\n",
      "Epoch [2], Step [190/595], Loss: 2.1773, Accuracy: 58.21%, Correct Predictions: 1106/1900\n",
      "Epoch [2], Step [200/595], Loss: 2.1772, Accuracy: 58.30%, Correct Predictions: 1166/2000\n",
      "Epoch [2], Step [210/595], Loss: 2.1776, Accuracy: 58.38%, Correct Predictions: 1226/2100\n",
      "Epoch [2], Step [220/595], Loss: 2.1775, Accuracy: 58.36%, Correct Predictions: 1284/2200\n",
      "Epoch [2], Step [230/595], Loss: 2.1773, Accuracy: 58.43%, Correct Predictions: 1344/2300\n",
      "Epoch [2], Step [240/595], Loss: 2.1771, Accuracy: 58.62%, Correct Predictions: 1407/2400\n",
      "Epoch [2], Step [250/595], Loss: 2.1772, Accuracy: 58.72%, Correct Predictions: 1468/2500\n",
      "Epoch [2], Step [260/595], Loss: 2.1772, Accuracy: 58.38%, Correct Predictions: 1518/2600\n",
      "Epoch [2], Step [270/595], Loss: 2.1774, Accuracy: 58.41%, Correct Predictions: 1577/2700\n",
      "Epoch [2], Step [280/595], Loss: 2.1775, Accuracy: 58.46%, Correct Predictions: 1637/2800\n",
      "Epoch [2], Step [290/595], Loss: 2.1777, Accuracy: 58.21%, Correct Predictions: 1688/2900\n",
      "Epoch [2], Step [300/595], Loss: 2.1777, Accuracy: 58.00%, Correct Predictions: 1740/3000\n",
      "Epoch [2], Step [310/595], Loss: 2.1775, Accuracy: 57.94%, Correct Predictions: 1796/3100\n",
      "Epoch [2], Step [320/595], Loss: 2.1776, Accuracy: 57.88%, Correct Predictions: 1852/3200\n",
      "Epoch [2], Step [330/595], Loss: 2.1775, Accuracy: 58.00%, Correct Predictions: 1914/3300\n",
      "Epoch [2], Step [340/595], Loss: 2.1775, Accuracy: 58.00%, Correct Predictions: 1972/3400\n",
      "Epoch [2], Step [350/595], Loss: 2.1776, Accuracy: 57.94%, Correct Predictions: 2028/3500\n",
      "Epoch [2], Step [360/595], Loss: 2.1775, Accuracy: 57.92%, Correct Predictions: 2085/3600\n",
      "Epoch [2], Step [370/595], Loss: 2.1776, Accuracy: 57.84%, Correct Predictions: 2140/3700\n",
      "Epoch [2], Step [380/595], Loss: 2.1776, Accuracy: 57.74%, Correct Predictions: 2194/3800\n",
      "Epoch [2], Step [390/595], Loss: 2.1775, Accuracy: 57.69%, Correct Predictions: 2250/3900\n",
      "Epoch [2], Step [400/595], Loss: 2.1774, Accuracy: 57.65%, Correct Predictions: 2306/4000\n",
      "Epoch [2], Step [410/595], Loss: 2.1773, Accuracy: 57.80%, Correct Predictions: 2370/4100\n",
      "Epoch [2], Step [420/595], Loss: 2.1773, Accuracy: 57.95%, Correct Predictions: 2434/4200\n",
      "Epoch [2], Step [430/595], Loss: 2.1772, Accuracy: 57.81%, Correct Predictions: 2486/4300\n",
      "Epoch [2], Step [440/595], Loss: 2.1771, Accuracy: 57.73%, Correct Predictions: 2540/4400\n",
      "Epoch [2], Step [450/595], Loss: 2.1771, Accuracy: 57.71%, Correct Predictions: 2597/4500\n",
      "Epoch [2], Step [460/595], Loss: 2.1772, Accuracy: 57.65%, Correct Predictions: 2652/4600\n",
      "Epoch [2], Step [470/595], Loss: 2.1772, Accuracy: 57.47%, Correct Predictions: 2701/4700\n",
      "Epoch [2], Step [480/595], Loss: 2.1772, Accuracy: 57.48%, Correct Predictions: 2759/4800\n",
      "Epoch [2], Step [490/595], Loss: 2.1773, Accuracy: 57.39%, Correct Predictions: 2812/4900\n",
      "Epoch [2], Step [500/595], Loss: 2.1774, Accuracy: 57.36%, Correct Predictions: 2868/5000\n",
      "Epoch [2], Step [510/595], Loss: 2.1775, Accuracy: 57.25%, Correct Predictions: 2920/5100\n",
      "Epoch [2], Step [520/595], Loss: 2.1774, Accuracy: 57.21%, Correct Predictions: 2975/5200\n",
      "Epoch [2], Step [530/595], Loss: 2.1774, Accuracy: 57.25%, Correct Predictions: 3034/5300\n",
      "Epoch [2], Step [540/595], Loss: 2.1775, Accuracy: 57.28%, Correct Predictions: 3093/5400\n",
      "Epoch [2], Step [550/595], Loss: 2.1777, Accuracy: 57.11%, Correct Predictions: 3141/5500\n",
      "Epoch [2], Step [560/595], Loss: 2.1778, Accuracy: 57.09%, Correct Predictions: 3197/5600\n",
      "Epoch [2], Step [570/595], Loss: 2.1778, Accuracy: 56.93%, Correct Predictions: 3245/5700\n",
      "Epoch [2], Step [580/595], Loss: 2.1778, Accuracy: 56.95%, Correct Predictions: 3303/5800\n",
      "Epoch [2], Step [590/595], Loss: 2.1777, Accuracy: 57.03%, Correct Predictions: 3365/5900\n",
      "Epoch [2] completed in 3.34 seconds. Average Loss: 2.1777, Accuracy: 57.05%\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "class LoRA_Scratch(nn.Module):\n",
    "    def __init__(self, layer, rank, alpha=0.5):\n",
    "        super().__init__()\n",
    "        feature_in, feature_out = layer.weight.shape\n",
    "\n",
    "        self.A = nn.Parameter(torch.zeros(feature_in, rank).to(device))\n",
    "        self.B = nn.Parameter(torch.zeros(rank, feature_out).to(device))\n",
    "        \n",
    "        self.scale = alpha / rank\n",
    "        self.LoRA = True\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.LoRA:\n",
    "            return X + (self.A @ self.B) * self.scale\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "def LoRA_convertor(layer, rank=1, lora_alpha=1):\n",
    "    return LoRA_Scratch(layer, rank=rank, alpha=lora_alpha)\n",
    "\n",
    "# Register LoRA to the layers\n",
    "parametrize.register_parametrization(model.layer1, \"weight\", LoRA_convertor(model.layer1))\n",
    "parametrize.register_parametrization(model.layer2, \"weight\", LoRA_convertor(model.layer2))\n",
    "parametrize.register_parametrization(model.layer3, \"weight\", LoRA_convertor(model.layer3))\n",
    "\n",
    "def Enable_lora(enable=True):\n",
    "    for Layer in [model.layer1, model.layer2, model.layer3]:\n",
    "        Layer.parametrizations['weight'][0].LoRA = enable\n",
    "        for param in [Layer.parametrizations['weight'][0].A, Layer.parametrizations['weight'][0].B]:\n",
    "            param.requires_grad = enable\n",
    "\n",
    "# Load the MNIST dataset, keeping only the digit 9\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with MNIST mean and std\n",
    "])\n",
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "exclude_indices = mnist_trainset.targets == 9\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define training function\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()            \n",
    "        \n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)   \n",
    "        loss.backward()                \n",
    "        optimizer.step()                 \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Count correct predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n",
    "            print(f'Epoch [{epoch}], Step [{batch_idx + 1}/{len(train_loader)}], '\n",
    "                  f'Loss: {running_loss / (batch_idx + 1):.4f}, '\n",
    "                  f'Accuracy: {100 * correct / total:.2f}%, '\n",
    "                  f'Correct Predictions: {correct}/{total}')\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f'Epoch [{epoch}] completed in {epoch_time:.2f} seconds. '\n",
    "          f'Average Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Set up optimizer and loss criterion\n",
    "\n",
    "# Training loop\n",
    "if __name__ == \"__main__\":\n",
    "    num_epochs = 2\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        Enable_lora(True)  # Enable LoRA during training\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "\n",
    "print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
